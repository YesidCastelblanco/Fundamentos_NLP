{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ENTORNO, LOGGING Y DEPENDENCIAS (KAGGLE)","metadata":{}},{"cell_type":"markdown","source":"# Integrantes:\n\n\n* Yesid Castelblanco\n* Albin Rivera\n","metadata":{}},{"cell_type":"markdown","source":"El presente notebook está orientado al desarrollo de un flujo completo de clasificación de texto en español empleando el modelo BERT dentro del ecosistema de Hugging Face Transformers. Su diseño está pensado para ejecutarse en entornos de experimentación como Kaggle, integrando desde la configuración del entorno y carga de datos hasta la optimización de hiperparámetros y el entrenamiento final del modelo.\n\nLa estructura se encuentra organizada en las siguientes secciones:\n\n\n* Preparación del entorno y dependencias.\n* Importación de librerías y componentes del pipeline.\n* Carga y exploración de datos en español.\n* Tokenización y construcción del modelo BERT.\n* Definición de métricas de evaluación (accuracy y F1).\n* Búsqueda de hiperparámetros con Optuna.\n* Entrenamiento final utilizando la mejor configuración encontrada.\n  ","metadata":{}},{"cell_type":"markdown","source":"# OBJETIVO","metadata":{}},{"cell_type":"markdown","source":"Implementar un pipeline reproducible y optimizado de clasificación de texto en español con BERT, que integre la carga de datos, el preprocesamiento mediante tokenización, la definición de métricas, la optimización de hiperparámetros y el entrenamiento final, garantizando un modelo con buen desempeño evaluado mediante accuracy y F1-score.","metadata":{}},{"cell_type":"markdown","source":"# DESARROLLO DEL TALLER","metadata":{}},{"cell_type":"markdown","source":"A continuacion, se prepara el entorno de ejecución del notebook asegurando su compatibilidad en Kaggle, Google Colab o localmente. Para ello, se configura un sistema de logging que centraliza mensajes en un archivo y silencia salidas innecesarias de librerías como TensorFlow. Además, detecta el entorno en que se ejecuta el script y ajusta variables en consecuencia, definiendo directorios de caché persistente en Kaggle para optimizar descargas de Hugging Face. De igual forma se implementan funciones que verifican e instalan versiones específicas de paquetes clave (transformers, datasets, accelerate, optuna, entre otros), garantizando estabilidad y reproducibilidad. También se descargan recursos lingüísticos de NLTK en español (stopwords y WordNet), y se aplican parámetros de optimización para el procesamiento eficiente de tokens y operaciones matriciales en PyTorch. En síntesis, este bloque asegura que todas las dependencias y configuraciones necesarias estén listas antes de iniciar el pipeline de clasificación de texto.","metadata":{}},{"cell_type":"code","source":"# ==== DEPLOY KAGGLE: LOGGING, ENV, INSTALLS ====\nimport os, sys, subprocess, warnings, logging, pkg_resources\n\n# Logging\nlogging.basicConfig(\n    filename='training.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\nlogging.getLogger(\"xla\").setLevel(logging.CRITICAL)\nlogging.getLogger(\"tensorflow\").setLevel(logging.CRITICAL)\nlogging.getLogger().setLevel(logging.CRITICAL)\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n    sys.stderr = open('/dev/null', 'w')\n\nIN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\nIN_COLAB = False\nif not IN_KAGGLE:\n    try:\n        import google.colab  # noqa\n        IN_COLAB = True\n        print(\"Ejecutando en Google Colab...\"); logger.info(\"Colab\")\n    except ImportError:\n        print(\"Ejecutando localmente...\"); logger.info(\"Local\")\nelse:\n    print(\"Ejecutando en Kaggle...\"); logger.info(\"Kaggle\")\n\ndef install_package(spec, extra_index_url=None):\n    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-warn-conflicts\", \"--upgrade\"]\n    if extra_index_url:\n        cmd += [\"--index-url\", extra_index_url]\n    cmd += spec.split()\n    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\ndef ver(pkg):\n    try:\n        return pkg_resources.get_distribution(pkg).version\n    except pkg_resources.DistributionNotFound:\n        return None\n\ndef ensure(pkg, target_version=None, spec=None, extra_index_url=None):\n    \"\"\"\n    - pkg: nombre para verificar (e.g., 'transformers')\n    - target_version: '4.45.2' para comprobar igualdad; si None solo verifica existencia\n    - spec: string completo pip (e.g., 'transformers==4.45.2 datasets==3.0.1')\n    \"\"\"\n    v = ver(pkg)\n    if (v is None) or (target_version and v != target_version):\n        install_package(spec if spec else pkg, extra_index_url)\n\n# Caché HF persistente en Kaggle\nif IN_KAGGLE:\n    cache_dir = \"/kaggle/working/hf_cache\"\n    os.makedirs(cache_dir, exist_ok=True)\n    os.environ[\"HF_HOME\"] = cache_dir\n    os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(cache_dir, \"datasets\")\n    os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(cache_dir, \"transformers\")\n\n# --- Paquetes clave (NO forzamos torch por defecto) ---\n# Nota: Kaggle ya trae torch+CUDA funcional. Solo si necesitas forzar:\nFORZAR_TORCH = False  # cambia a True si realmente lo necesitas\n\nif (IN_COLAB or IN_KAGGLE) and FORZAR_TORCH:\n    # CUDA 11.8 (suele ser compatible con P100 en Kaggle)\n    ensure(\"torch\", target_version=\"2.3.1\",  # versión estable conocida; ajusta si lo requieres\n           spec=\"torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\",\n           extra_index_url=\"https://download.pytorch.org/whl/cu118\")\n\n# Core HF y ciencia de datos\nensure(\"transformers\", \"4.45.2\", spec=\"transformers==4.45.2\")\nensure(\"datasets\", \"3.0.1\", spec=\"datasets==3.0.1\")\nensure(\"accelerate\", \"0.34.2\", spec=\"accelerate==0.34.2\")\nensure(\"evaluate\", \"0.4.3\", spec=\"evaluate==0.4.3\")\nensure(\"sentence-transformers\", \"3.2.1\", spec=\"sentence-transformers==3.2.1\")\nensure(\"peft\", \"0.13.2\", spec=\"peft==0.13.2\")\nensure(\"nltk\", \"3.9.1\", spec=\"nltk==3.9.1\")\nensure(\"nlpaug\", \"1.1.11\", spec=\"nlpaug==1.1.11\")\nensure(\"optuna\", \"3.6.1\", spec=\"optuna==3.6.1\")\n\n# (Opcionales, por si tu pipeline los usa)\nensure(\"numpy\", \"1.26.4\", spec=\"numpy==1.26.4\")\nensure(\"pandas\", \"2.2.2\", spec=\"pandas==2.2.2\")\nensure(\"pyarrow\", \"15.0.2\", spec=\"pyarrow==15.0.2\")\n\n# Verificación rápida\ntry:\n    import torch, transformers, datasets, evaluate, accelerate, optuna, nltk\nexcept ImportError as e:\n    logger.error(f\"Import error: {e}\")\n    raise\n\n# NLTK y entorno\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\n# Este tagger inglés no es necesario para ES; lo dejamos por compat:\ntry:\n    nltk.download('averaged_perceptron_tagger_eng', quiet=True)\nexcept Exception:\n    pass\n\nstop_words = set(stopwords.words('spanish'))\n\n# Silencio y precisión\nfrom transformers.utils import logging as hf_logging\nwarnings.filterwarnings('ignore')\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nhf_logging.set_verbosity_error()\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/usr/lib/cuda\"\ntry:\n    torch.set_float32_matmul_precision('medium')\nexcept Exception:\n    pass\n\nprint(\"Entorno listo ✅\")\nprint(\"Versions -> torch:\", torch.__version__,\n      \"| transformers:\", transformers.__version__,\n      \"| datasets:\", datasets.__version__,\n      \"| accelerate:\", accelerate.__version__,\n      \"| evaluate:\", evaluate.__version__)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T19:54:38.508104Z","iopub.execute_input":"2025-09-13T19:54:38.508500Z","iopub.status.idle":"2025-09-13T19:54:50.065817Z","shell.execute_reply.started":"2025-09-13T19:54:38.508461Z","shell.execute_reply":"2025-09-13T19:54:50.065027Z"}},"outputs":[{"name":"stdout","text":"Ejecutando en Kaggle...\n","output_type":"stream"},{"name":"stderr","text":"2025-09-13 19:54:45.002767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757793285.027247     185 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757793285.034467     185 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Entorno listo ✅\nVersions -> torch: 2.6.0+cu124 | transformers: 4.45.2 | datasets: 3.0.1 | accelerate: 0.34.2 | evaluate: 0.4.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 1) IMPORTS DEL PIPELINE","metadata":{}},{"cell_type":"markdown","source":"A continuacion se implementan las librerías requeridas para la construcción del flujo de clasificación de texto. Se importan módulos de Hugging Face para el preprocesamiento y entrenamiento del modelo (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding) y la utilidad load_dataset para acceder a conjuntos de datos. Además, se incluyen librerías de apoyo como numpy, evaluate y random, que permiten manejar cálculos numéricos, métricas y reproducibilidad en los experimentos. Finalmente, se define la variable DEVICE que detecta si hay una GPU disponible para ejecutar el entrenamiento en CUDA, lo cual optimiza el rendimiento, o en caso contrario se recurre a la CPU. En resumen, este bloque inicializa las herramientas clave que sostendrán las etapas de carga, tokenización, entrenamiento y evaluación del modelo.","metadata":{}},{"cell_type":"code","source":"# ============================\n# 1) IMPORTS DEL PIPELINE\n# ============================\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    TrainingArguments, Trainer, DataCollatorWithPadding\n)\nimport numpy as np\nimport evaluate\nimport random\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Dispositivo:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T19:54:50.067188Z","iopub.execute_input":"2025-09-13T19:54:50.067731Z","iopub.status.idle":"2025-09-13T19:54:50.372509Z","shell.execute_reply.started":"2025-09-13T19:54:50.067704Z","shell.execute_reply":"2025-09-13T19:54:50.371719Z"}},"outputs":[{"name":"stdout","text":"Dispositivo: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# 2) CARGA DE DATOS (ESPAÑOL)","metadata":{}},{"cell_type":"markdown","source":"Aqui se lleva a cabo la carga de datos donde se utiliza el conjunto de reseñas en español de Amazon Reviews Multi disponible en Hugging Face. Esta base de datos contiene opiniones de clientes en español, donde cada texto está asociado a una etiqueta de sentimiento representada en escala de 0 a 4, que equivale a calificaciones de 1 a 5 estrellas (desde muy negativa hasta muy positiva). El código descarga el dataset desde \"SetFit/amazon_reviews_multi_es\" y lo organiza en las particiones estándar: entrenamiento, validación y prueba. Para facilitar los experimentos y optimizar el uso de recursos (tiempo y memoria de GPU), se implementa un submuestreo que reduce la cantidad de ejemplos en este caso de 8.000 para entrenamiento y 2.000 para validación, manteniendo la totalidad del conjunto de prueba. De esta manera, se asegura que el pipeline pueda probarse de forma ágil sin comprometer la calidad del entrenamiento. ","metadata":{}},{"cell_type":"code","source":"# ============================\n# 2) CARGA DE DATOS (ESPAÑOL)\n# ============================\n# Dataset de reseñas en español (etiquetas 0..4 ~ 1..5 estrellas)\nds = load_dataset(\"SetFit/amazon_reviews_multi_es\", cache_dir=os.environ.get(\"HF_DATASETS_CACHE\", None))\nprint(ds)\nprint(ds[\"train\"][0])\n\n# (Opcional) Submuestreo para búsq. rápida de HP; ajusta según tiempo/VRAM:\nSUBSET_TRAIN = 8000    # puedes subir a 20000 si la P100 te da margen\nSUBSET_VAL   = 2000\ntrain_ds = ds[\"train\"].shuffle(seed=42).select(range(min(SUBSET_TRAIN, len(ds[\"train\"]))))\nval_ds   = ds[\"validation\"].shuffle(seed=42).select(range(min(SUBSET_VAL, len(ds[\"validation\"]))))\ntest_ds  = ds[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T19:54:50.373459Z","iopub.execute_input":"2025-09-13T19:54:50.373881Z","iopub.status.idle":"2025-09-13T19:54:51.147670Z","shell.execute_reply.started":"2025-09-13T19:54:50.373854Z","shell.execute_reply":"2025-09-13T19:54:51.147000Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'label', 'label_text'],\n        num_rows: 200000\n    })\n    validation: Dataset({\n        features: ['id', 'text', 'label', 'label_text'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['id', 'text', 'label', 'label_text'],\n        num_rows: 5000\n    })\n})\n{'id': 'es_0491108', 'text': 'Nada bueno se me fue ka pantalla en menos de 8 meses y no he recibido respuesta del fabricante', 'label': 0, 'label_text': '0'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 3) TOKENIZACIÓN Y MODELO","metadata":{}},{"cell_type":"markdown","source":"Para esta etapa,se emplea como base el modelo BETO, una versión de BERT entrenada específicamente para español. Con su tokenizer, los textos del dataset se transforman en secuencias de tokens con truncamiento a un máximo de 256, aplicándose sobre los conjuntos de entrenamiento, validación y prueba. Al mismo tiempo, se conserva únicamente la información relevante de cada registro (text y label). Se calcula el número de clases a partir de las etiquetas y se define un data collator con padding dinámico, lo que permite que los lotes tengan la misma longitud y optimicen el uso de GPU. En síntesis, este bloque estandariza los datos textuales y establece los insumos para que BETO pueda entrenarse en la clasificación de reseñas en español.","metadata":{}},{"cell_type":"code","source":"# ============================\n# 3) TOKENIZACIÓN Y MODELO\n# ============================\nMODEL_NAME = \"dccuchile/bert-base-spanish-wwm-cased\"  # BETO\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\", None))\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n\ntok_train = train_ds.map(tokenize, batched=True, remove_columns=[c for c in train_ds.column_names if c not in [\"text\", \"label\"]])\ntok_val   = val_ds.map(tokenize, batched=True, remove_columns=[c for c in val_ds.column_names if c not in [\"text\", \"label\"]])\ntok_test  = test_ds.map(tokenize, batched=True, remove_columns=[c for c in test_ds.column_names if c not in [\"text\", \"label\"]])\n\nnum_labels = len(set(ds[\"train\"][\"label\"]))\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T19:54:51.149329Z","iopub.execute_input":"2025-09-13T19:54:51.149570Z","iopub.status.idle":"2025-09-13T19:54:51.665600Z","shell.execute_reply.started":"2025-09-13T19:54:51.149548Z","shell.execute_reply":"2025-09-13T19:54:51.665036Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abc7fb6980f347da8160811dd860bb52"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# 4) MÉTRICAS (ACC y F1 weighted)","metadata":{}},{"cell_type":"markdown","source":"Se define las métricas de evaluación para el modelo, las cuales son accuracy (precisión global) y F1-score con promedio ponderado, que considera el desempeño de cada clase en función de su peso en el dataset. La función compute_metrics toma como entrada las predicciones del modelo (logits) y las etiquetas reales, convierte los logits en clases predichas mediante argmax, y luego calcula los valores de ambas métricas. De esta manera, se obtiene una evaluación equilibrada, donde la accuracy mide qué tan a menudo el modelo acierta en general, mientras que el F1 ponderado refleja la calidad de la clasificación en datasets con múltiples clases, ajustando el impacto de clases más frecuentes y menos frecuentes. ","metadata":{}},{"cell_type":"code","source":"# ============================\n# 4) MÉTRICAS (ACC y F1 weighted)\n# ============================\nacc = evaluate.load(\"accuracy\")\nf1w = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return {\n        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n        \"f1\": f1w.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T19:54:51.666394Z","iopub.execute_input":"2025-09-13T19:54:51.666708Z","iopub.status.idle":"2025-09-13T19:54:52.148892Z","shell.execute_reply.started":"2025-09-13T19:54:51.666664Z","shell.execute_reply":"2025-09-13T19:54:52.148320Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Este bloque ejecuta la limpieza de directorios temporales y de salida en el entorno de Kaggle. El primer comando (!rm -rf /kaggle/working/hf_cache) elimina la caché de Hugging Face almacenada durante ejecuciones previas, evitando que queden restos de modelos o datasets que puedan generar conflictos. El segundo (!rm -rf outputs optuna_search) borra las carpetas de resultados y de búsquedas de hiperparámetros de Optuna, garantizando que las siguientes corridas del notebook empiecen desde cero y con un entorno limpio. En resumen, este paso asegura que el entrenamiento y la optimización se realicen sin residuos de ejecuciones anteriores, lo cual favorece la reproducibilidad y la consistencia de los experimentos.","metadata":{}},{"cell_type":"code","source":"!rm -rf /kaggle/working/hf_cache\n!rm -rf outputs optuna_search\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T20:38:43.941411Z","iopub.execute_input":"2025-09-13T20:38:43.941862Z","iopub.status.idle":"2025-09-13T20:38:44.357401Z","shell.execute_reply.started":"2025-09-13T20:38:43.941835Z","shell.execute_reply":"2025-09-13T20:38:44.356474Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# 5) BÚSQUEDA DE HIPERPARÁMETROS (OPTUNA)","metadata":{}},{"cell_type":"markdown","source":"Aqui se realiza la optimización de hiperparámetros con Optuna para mejorar el desempeño del modelo BETO en la clasificación de reseñas. Se define una función de métricas (accuracy y F1 ponderado), una inicialización del modelo y un conjunto de argumentos de entrenamiento simplificados. Para agilizar el proceso, se emplea un subset reducido de datos. Optuna explora combinaciones de tasa de aprendizaje, batch size, número de épocas y weight decay en varias pruebas, y al final se reporta la mejor configuración encontrada.","metadata":{}},{"cell_type":"code","source":"# ============================\n# 5) BÚSQUEDA DE HIPERPARÁMETROS (OPTUNA)\n# ============================\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n    }\n\ndef model_init():\n    return AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME,\n        num_labels=num_labels,\n        cache_dir=os.environ.get(\"TRANSFORMERS_CACHE\", None)\n    )\n\n# Argumentos base: no guardar checkpoints, carpeta temporal\nsearch_args = TrainingArguments(\n    output_dir=\"/kaggle/temp/optuna_search\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"no\",\n    load_best_model_at_end=False,\n    report_to=\"none\",\n    fp16=True,\n    gradient_accumulation_steps=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    logging_steps=50,\n    seed=42\n)\n\n# Subset para búsqueda rápida\ntok_train_small = tok_train.shuffle(seed=42).select(range(2000))\ntok_val_small   = tok_val.shuffle(seed=42).select(range(500))\n\nsearch_trainer = Trainer(\n    model_init=model_init,\n    args=search_args,\n    train_dataset=tok_train_small,\n    eval_dataset=tok_val_small,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\nbest_run = search_trainer.hyperparameter_search(\n    direction=\"maximize\",\n    backend=\"optuna\",\n    n_trials=6,  # baja a 6-8 para no llenar disco\n    hp_space=lambda trial: {\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 4),\n        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n    }\n)\n\nprint(\"Mejor ejecución (Optuna):\", best_run)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T20:38:48.558217Z","iopub.execute_input":"2025-09-13T20:38:48.558544Z","iopub.status.idle":"2025-09-13T20:46:17.051645Z","shell.execute_reply.started":"2025-09-13T20:38:48.558507Z","shell.execute_reply":"2025-09-13T20:46:17.050984Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"967f2e3f275e47c4ad6d52a0d599f74a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e70397fe8f0a4bb0b64560050a5496f3"}},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 1.2298344373703003, 'eval_accuracy': 0.434, 'eval_f1': 0.39799723533216663, 'eval_runtime': 1.6944, 'eval_samples_per_second': 295.093, 'eval_steps_per_second': 18.886, 'epoch': 0.9841269841269841}\n{'loss': 1.3366, 'grad_norm': 4.5296406745910645, 'learning_rate': 1.0213608806609128e-05, 'epoch': 1.5873015873015874}\n{'eval_loss': 1.1346220970153809, 'eval_accuracy': 0.478, 'eval_f1': 0.467624630213346, 'eval_runtime': 1.6975, 'eval_samples_per_second': 294.556, 'eval_steps_per_second': 18.852, 'epoch': 2.0}\n{'eval_loss': 1.129022240638733, 'eval_accuracy': 0.484, 'eval_f1': 0.46446599113111553, 'eval_runtime': 1.6973, 'eval_samples_per_second': 294.592, 'eval_steps_per_second': 18.854, 'epoch': 2.9523809523809526}\n{'train_runtime': 77.8079, 'train_samples_per_second': 77.113, 'train_steps_per_second': 1.195, 'train_loss': 1.1881442736553889, 'epoch': 2.9523809523809526}\n{'loss': 1.3493, 'grad_norm': 6.630311012268066, 'learning_rate': 3.0175304438273315e-05, 'epoch': 0.8}\n{'eval_loss': 1.1643760204315186, 'eval_accuracy': 0.464, 'eval_f1': 0.4241864828025253, 'eval_runtime': 1.7068, 'eval_samples_per_second': 292.951, 'eval_steps_per_second': 18.749, 'epoch': 0.992}\n{'loss': 1.0578, 'grad_norm': 5.970038414001465, 'learning_rate': 1.9162419606786702e-05, 'epoch': 1.6}\n{'eval_loss': 1.1058685779571533, 'eval_accuracy': 0.508, 'eval_f1': 0.5069902732759363, 'eval_runtime': 1.7051, 'eval_samples_per_second': 293.24, 'eval_steps_per_second': 18.767, 'epoch': 2.0}\n{'loss': 0.8358, 'grad_norm': 5.082130432128906, 'learning_rate': 8.149534775300092e-06, 'epoch': 2.4}\n{'eval_loss': 1.1442792415618896, 'eval_accuracy': 0.506, 'eval_f1': 0.5012855020812362, 'eval_runtime': 1.7077, 'eval_samples_per_second': 292.791, 'eval_steps_per_second': 18.739, 'epoch': 2.976}\n{'train_runtime': 70.7407, 'train_samples_per_second': 84.817, 'train_steps_per_second': 2.629, 'train_loss': 1.0101069481142106, 'epoch': 2.976}\n{'loss': 1.4275, 'grad_norm': 4.639593124389648, 'learning_rate': 1.4841897405397837e-05, 'epoch': 0.8}\n{'eval_loss': 1.1678603887557983, 'eval_accuracy': 0.448, 'eval_f1': 0.415169142559379, 'eval_runtime': 1.7181, 'eval_samples_per_second': 291.012, 'eval_steps_per_second': 18.625, 'epoch': 0.992}\n{'loss': 1.123, 'grad_norm': 6.646627426147461, 'learning_rate': 9.38531747694275e-06, 'epoch': 1.6}\n{'eval_loss': 1.105563998222351, 'eval_accuracy': 0.488, 'eval_f1': 0.48355351070976316, 'eval_runtime': 1.7178, 'eval_samples_per_second': 291.071, 'eval_steps_per_second': 18.629, 'epoch': 2.0}\n{'loss': 0.9533, 'grad_norm': 5.451622009277344, 'learning_rate': 3.928737548487663e-06, 'epoch': 2.4}\n{'eval_loss': 1.1020619869232178, 'eval_accuracy': 0.498, 'eval_f1': 0.48776771187449647, 'eval_runtime': 1.7273, 'eval_samples_per_second': 289.477, 'eval_steps_per_second': 18.527, 'epoch': 2.976}\n{'train_runtime': 70.9653, 'train_samples_per_second': 84.548, 'train_steps_per_second': 2.621, 'train_loss': 1.111850841071016, 'epoch': 2.976}\n{'loss': 1.4983, 'grad_norm': 4.807109832763672, 'learning_rate': 9.89121702975209e-06, 'epoch': 0.8}\n{'eval_loss': 1.2326641082763672, 'eval_accuracy': 0.436, 'eval_f1': 0.38363318626442344, 'eval_runtime': 1.7042, 'eval_samples_per_second': 293.387, 'eval_steps_per_second': 18.777, 'epoch': 0.992}\n{'loss': 1.1841, 'grad_norm': 6.456786632537842, 'learning_rate': 7.393434951531867e-06, 'epoch': 1.6}\n{'eval_loss': 1.1249477863311768, 'eval_accuracy': 0.466, 'eval_f1': 0.45307502078204537, 'eval_runtime': 1.7111, 'eval_samples_per_second': 292.207, 'eval_steps_per_second': 18.701, 'epoch': 2.0}\n{'loss': 1.0279, 'grad_norm': 5.81807279586792, 'learning_rate': 4.895652873311641e-06, 'epoch': 2.4}\n{'eval_loss': 1.096005916595459, 'eval_accuracy': 0.502, 'eval_f1': 0.49392574378001086, 'eval_runtime': 1.7075, 'eval_samples_per_second': 292.83, 'eval_steps_per_second': 18.741, 'epoch': 2.992}\n{'loss': 0.9555, 'grad_norm': 8.028385162353516, 'learning_rate': 2.4478264366558206e-06, 'epoch': 3.2}\n{'eval_loss': 1.1081345081329346, 'eval_accuracy': 0.494, 'eval_f1': 0.48174973787719844, 'eval_runtime': 1.7084, 'eval_samples_per_second': 292.672, 'eval_steps_per_second': 18.731, 'epoch': 3.968}\n{'train_runtime': 94.1303, 'train_samples_per_second': 84.989, 'train_steps_per_second': 2.635, 'train_loss': 1.1141228060568533, 'epoch': 3.968}\n{'eval_loss': 1.179678201675415, 'eval_accuracy': 0.454, 'eval_f1': 0.42681790241257417, 'eval_runtime': 1.7089, 'eval_samples_per_second': 292.589, 'eval_steps_per_second': 18.726, 'epoch': 0.9841269841269841}\n{'loss': 1.2806, 'grad_norm': 4.822757244110107, 'learning_rate': 1.9785440286678336e-05, 'epoch': 1.5873015873015874}\n{'eval_loss': 1.0986593961715698, 'eval_accuracy': 0.506, 'eval_f1': 0.49996683531714065, 'eval_runtime': 1.7107, 'eval_samples_per_second': 292.271, 'eval_steps_per_second': 18.705, 'epoch': 2.0}\n{'eval_loss': 1.1229358911514282, 'eval_accuracy': 0.534, 'eval_f1': 0.5290631864453651, 'eval_runtime': 1.6944, 'eval_samples_per_second': 295.089, 'eval_steps_per_second': 18.886, 'epoch': 2.984126984126984}\n{'loss': 0.8995, 'grad_norm': 4.685993194580078, 'learning_rate': 6.416899552436217e-06, 'epoch': 3.1746031746031744}\n{'eval_loss': 1.131466269493103, 'eval_accuracy': 0.52, 'eval_f1': 0.5145532944656958, 'eval_runtime': 1.7046, 'eval_samples_per_second': 293.322, 'eval_steps_per_second': 18.773, 'epoch': 3.9365079365079367}\n{'train_runtime': 103.418, 'train_samples_per_second': 77.356, 'train_steps_per_second': 1.199, 'train_loss': 1.0253363886187155, 'epoch': 3.9365079365079367}\n{'loss': 1.4777, 'grad_norm': 4.4804253578186035, 'learning_rate': 1.1483244127143783e-05, 'epoch': 0.8}\n{'eval_loss': 1.2059857845306396, 'eval_accuracy': 0.446, 'eval_f1': 0.39992036784804674, 'eval_runtime': 1.7077, 'eval_samples_per_second': 292.789, 'eval_steps_per_second': 18.739, 'epoch': 0.992}\nMejor ejecución (Optuna): BestRun(run_id='4', objective=1.0345532944656957, hyperparameters={'learning_rate': 3.3153981020920456e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4, 'weight_decay': 0.17161219885330212}, run_summary=None)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"La búsqueda de hiperparámetros con Optuna arrojó múltiples ejecuciones con diferentes combinaciones de tasa de aprendizaje, tamaño de batch, número de épocas y weight decay, registrando métricas de exactitud y F1 ponderado en cada caso. Tras las pruebas, la mejor configuración encontrada fue un learning rate cercano a 3.3e-5, un batch size de 32, 4 épocas de entrenamiento y un weight decay de aproximadamente 0.17, parámetros que permitieron alcanzar el mejor equilibrio entre accuracy y F1-score en la tarea de clasificación de reseñas en español.","metadata":{}},{"cell_type":"markdown","source":"# 6) ENTRENAMIENTO FINAL CON LOS MEJORES HP","metadata":{}},{"cell_type":"markdown","source":"Aqui se lleva a cabo el entrenamiento final del modelo, utilizando los hiperparámetros óptimos hallados en la búsqueda con Optuna. Primero, se aplican estos valores directamente a los argumentos del trainer, garantizando que el proceso de ajuste se realice con la mejor configuración encontrada. Luego, se actualiza la función de métricas (compute_metrics) para que opere con la versión definida previamente. Finalmente, se ejecuta el entrenamiento completo sobre todo el dataset, no solo con subsets como en la fase de prueba, con lo cual el modelo aprovecha la totalidad de los datos disponibles para aprender y lograr un mejor rendimiento en la tarea de clasificación de reseñas en español. ","metadata":{}},{"cell_type":"code","source":"# ============================\n# 6) ENTRENAMIENTO FINAL CON LOS MEJORES HP\n# ============================\n\n# Aplicar hiperparámetros óptimos al trainer\nfor n, v in best_run.hyperparameters.items():\n    setattr(trainer.args, n, v)\n\nprint(\"Hiperparámetros finales:\", trainer.args)\nlogger.info(f\"Final Training Args: {trainer.args}\")\n\n# Reemplazar compute_metrics por la versión sin evaluate\ntrainer.compute_metrics = compute_metrics\n\n# Entrenamiento final completo en todo el dataset\ntrainer.train()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:01:23.863825Z","iopub.execute_input":"2025-09-13T21:01:23.864443Z","iopub.status.idle":"2025-09-13T21:08:22.900600Z","shell.execute_reply.started":"2025-09-13T21:01:23.864413Z","shell.execute_reply":"2025-09-13T21:08:22.900014Z"}},"outputs":[{"name":"stdout","text":"Hiperparámetros finales: TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=True,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=None,\neval_strategy=IntervalStrategy.EPOCH,\neval_use_gather_object=False,\nevaluation_strategy=epoch,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=2,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=HubStrategy.EVERY_SAVE,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=3.3153981020920456e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=outputs/runs/Sep13_19-54-52_cc6892314917,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=50,\nlogging_strategy=IntervalStrategy.STEPS,\nlr_scheduler_kwargs={},\nlr_scheduler_type=SchedulerType.LINEAR,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=f1,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=4,\noptim=OptimizerNames.ADAMW_TORCH,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=outputs,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=16,\nper_device_train_batch_size=32,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=[],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=outputs,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=IntervalStrategy.EPOCH,\nsave_total_limit=None,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.17161219885330212,\n)\n{'loss': 1.3086, 'grad_norm': 6.649620532989502, 'learning_rate': 2.9838582918828412e-05, 'epoch': 0.4}\n{'loss': 1.0966, 'grad_norm': 5.115312099456787, 'learning_rate': 2.6523184816736368e-05, 'epoch': 0.8}\n{'eval_loss': 1.0487277507781982, 'eval_accuracy': 0.525, 'eval_f1': 0.5225197641626191, 'eval_runtime': 6.7471, 'eval_samples_per_second': 296.425, 'eval_steps_per_second': 18.527, 'epoch': 1.0}\n{'loss': 1.004, 'grad_norm': 5.032232761383057, 'learning_rate': 2.3207786714644317e-05, 'epoch': 1.2}\n{'loss': 0.9342, 'grad_norm': 6.736208438873291, 'learning_rate': 1.9892388612552272e-05, 'epoch': 1.6}\n{'loss': 0.9079, 'grad_norm': 5.6906046867370605, 'learning_rate': 1.6576990510460228e-05, 'epoch': 2.0}\n{'eval_loss': 1.067078709602356, 'eval_accuracy': 0.5335, 'eval_f1': 0.5396650632783553, 'eval_runtime': 6.7945, 'eval_samples_per_second': 294.357, 'eval_steps_per_second': 18.397, 'epoch': 2.0}\n{'loss': 0.779, 'grad_norm': 4.949689865112305, 'learning_rate': 1.3261592408368184e-05, 'epoch': 2.4}\n{'loss': 0.7557, 'grad_norm': 8.81569766998291, 'learning_rate': 9.946194306276136e-06, 'epoch': 2.8}\n{'eval_loss': 1.1153029203414917, 'eval_accuracy': 0.5375, 'eval_f1': 0.5362139152523386, 'eval_runtime': 6.7307, 'eval_samples_per_second': 297.147, 'eval_steps_per_second': 18.572, 'epoch': 3.0}\n{'loss': 0.6754, 'grad_norm': 6.7535223960876465, 'learning_rate': 6.630796204184092e-06, 'epoch': 3.2}\n{'loss': 0.6193, 'grad_norm': 9.108163833618164, 'learning_rate': 3.315398102092046e-06, 'epoch': 3.6}\n{'loss': 0.6248, 'grad_norm': 5.5149712562561035, 'learning_rate': 0.0, 'epoch': 4.0}\n{'eval_loss': 1.1844252347946167, 'eval_accuracy': 0.529, 'eval_f1': 0.529160617165837, 'eval_runtime': 6.7448, 'eval_samples_per_second': 296.527, 'eval_steps_per_second': 18.533, 'epoch': 4.0}\n{'train_runtime': 417.9748, 'train_samples_per_second': 76.56, 'train_steps_per_second': 1.196, 'train_loss': 0.8705581359863281, 'epoch': 4.0}\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=0.8705581359863281, metrics={'train_runtime': 417.9748, 'train_samples_per_second': 76.56, 'train_steps_per_second': 1.196, 'train_loss': 0.8705581359863281, 'epoch': 4.0})"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# 7) EVALUACIÓN EN TEST","metadata":{}},{"cell_type":"markdown","source":"Aqui se realiza la evaluación final del modelo en el conjunto de prueba (test) y se visualiza el resultado de entrenamiento. Primero, se ejecuta trainer.evaluate(tok_test) para medir el desempeño del modelo sobre datos nunca vistos, obteniendo las métricas finales de exactitud y F1 ponderado. Estas métricas son impresas y registradas en los logs, lo que permite validar de manera objetiva la capacidad de generalización del modelo. Luego, se guarda el modelo entrenado junto con su tokenizer en la carpeta outputs/best_model, lo que facilita reutilizarlo en tareas de inferencia o despliegue, posteriormente sin necesidad de reentrenar. En síntesis, este bloque valida el rendimiento definitivo alcanzado y almacena el modelo listo para tareas de inferencia o despliegue.","metadata":{}},{"cell_type":"code","source":"# ============================\n# 7) EVALUACIÓN EN TEST\n# ============================\nmetrics = trainer.evaluate(tok_test)\nprint(\"Métricas finales en test:\", metrics)\nlogger.info(f\"Test metrics: {metrics}\")\n\n# Guardar el mejor modelo\nsave_dir = \"outputs/best_model\"\ntrainer.save_model(save_dir)\ntokenizer.save_pretrained(save_dir)\nprint(f\"Modelo guardado en: {save_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:15:20.942729Z","iopub.execute_input":"2025-09-13T21:15:20.943084Z","iopub.status.idle":"2025-09-13T21:15:38.573230Z","shell.execute_reply.started":"2025-09-13T21:15:20.943059Z","shell.execute_reply":"2025-09-13T21:15:38.572335Z"}},"outputs":[{"name":"stdout","text":"{'eval_loss': 1.0524392127990723, 'eval_accuracy': 0.539, 'eval_f1': 0.5451025028597957, 'eval_runtime': 17.0144, 'eval_samples_per_second': 293.868, 'eval_steps_per_second': 18.396, 'epoch': 4.0}\nMétricas finales en test: {'eval_loss': 1.0524392127990723, 'eval_accuracy': 0.539, 'eval_f1': 0.5451025028597957, 'eval_runtime': 17.0144, 'eval_samples_per_second': 293.868, 'eval_steps_per_second': 18.396, 'epoch': 4.0}\nModelo guardado en: outputs/best_model\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Los resultados de la evaluación final en el conjunto de prueba muestran que el modelo alcanzó una exactitud (accuracy) de 0.539 y un F1 ponderado de 0.545, con una pérdida en validación de aproximadamente 1.05. Estos valores indican un rendimiento moderado, es decir que el modelo logra clasificar correctamente algo más de la mitad de los ejemplos y mantiene un equilibrio aceptable entre precisión y cobertura en las distintas clases.","metadata":{}},{"cell_type":"markdown","source":"# 8) PROBAR EL MODELO CON TEXTO NUEVO","metadata":{}},{"cell_type":"markdown","source":"Finalmente se implementa la fase de inferencia del pipeline, es decir, probar el modelo ya entrenado con textos nuevos. Para ello, se carga el modelo y el tokenizer desde la carpeta outputs/best_model usando la función pipeline de Hugging Face en modo de clasificación de texto. Luego, se definen algunos ejemplos en español que representan reseñas positivas, negativas y neutras. El modelo procesa estos textos y genera predicciones de clase (LABEL_X) junto con un puntaje de confianza. Posteriormente, estas etiquetas numéricas se convierten en un formato más intuitivo de estrellas (de 1 a 5), lo que facilita la interpretación del resultado. En síntesis, este bloque valida de forma práctica el funcionamiento del modelo entrenado, mostrando cómo traduce reseñas escritas en español en calificaciones de estrellas con su respectivo nivel de confianza.","metadata":{}},{"cell_type":"code","source":"# ============================\n# 8) PROBAR EL MODELO CON TEXTO NUEVO\n# ============================\n\nfrom transformers import pipeline\n\n# Ruta del modelo guardado\nMODEL_DIR = \"outputs/best_model\"\n\n# Cargar pipeline de clasificación\nclassifier = pipeline(\"text-classification\", model=MODEL_DIR, tokenizer=MODEL_DIR, device=0)\n\n# Ejemplos de prueba\nejemplos = [\n    \"Este producto es excelente, llegó rápido y funciona muy bien\",\n    \"El paquete vino roto y el producto no sirve\",\n    \"Está bien, cumple pero nada especial\"\n]\n\n# Obtener predicciones\npredicciones = classifier(ejemplos)\n\n# Mapear LABEL_X a estrellas\nlabel2stars = {0: \"⭐ (1 estrella)\", 1: \"⭐⭐\", 2: \"⭐⭐⭐\", 3: \"⭐⭐⭐⭐\", 4: \"⭐⭐⭐⭐⭐\"}\n\n# Mostrar resultados\nfor texto, pred in zip(ejemplos, predicciones):\n    label_num = int(pred['label'].split(\"_\")[-1])\n    print(f\"Texto: {texto}\\n→ Predicción: {label2stars[label_num]} (confianza: {pred['score']:.2f})\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:15:45.957488Z","iopub.execute_input":"2025-09-13T21:15:45.958322Z","iopub.status.idle":"2025-09-13T21:15:46.340494Z","shell.execute_reply.started":"2025-09-13T21:15:45.958293Z","shell.execute_reply":"2025-09-13T21:15:46.339801Z"}},"outputs":[{"name":"stdout","text":"Texto: Este producto es excelente, llegó rápido y funciona muy bien\n→ Predicción: ⭐⭐⭐⭐⭐ (confianza: 0.75)\n\nTexto: El paquete vino roto y el producto no sirve\n→ Predicción: ⭐ (1 estrella) (confianza: 0.86)\n\nTexto: Está bien, cumple pero nada especial\n→ Predicción: ⭐⭐⭐ (confianza: 0.69)\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Los resultados de prueba muestran que el modelo logra identificar adecuadamente el sentimiento de los textos nuevos; a la reseña positiva “Este producto es excelente, llegó rápido y funciona muy bien” le asigna 5 estrellas con una confianza del 0.75; a la reseña negativa “El paquete vino roto y el producto no sirve” la clasifica con 1 estrella y una alta confianza de 0.86; y a la reseña más neutra “Está bien, cumple pero nada especial” la ubica en 3 estrellas con confianza de 0.69. Esto evidencia que el modelo puede diferenciar entre opiniones positivas, negativas y moderadas, generando predicciones coherentes con el sentido general de cada texto.","metadata":{}},{"cell_type":"markdown","source":"# 9) CONCLUSIONES","metadata":{}},{"cell_type":"markdown","source":"* La búsqueda de hiperparámetros con Optuna permitió encontrar una configuración adecuada (lr ≈ 3.3e-5, batch size 32, 4 épocas, weight decay ≈ 0.17), la cual se aplicó en el entrenamiento final para maximizar las métricas de desempeño.\n* En el conjunto de prueba, el modelo alcanzó un accuracy de 0.539 y un F1 ponderado de 0.545, lo que demuestra que logra una clasificación moderada de las reseñas, cumpliendo con el objetivo de obtener un modelo funcional aunque con margen de mejora en la generalización.\n* Las pruebas con ejemplos nuevos evidenciaron que el modelo es capaz de asignar predicciones coherentes con el sentimiento de los textos, diferenciando entre opiniones positivas, negativas y neutras, lo que valida su aplicabilidad en escenarios reales.\n* El modelo final fue guardado junto con su tokenizer, quedando listo para ser reutilizado en tareas de inferencia o integrarse en aplicaciones prácticas de análisis de reseñas, cerrando el ciclo planteado en el objetivo inicial.","metadata":{}}]}