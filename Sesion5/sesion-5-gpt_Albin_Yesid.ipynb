{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Optimización de modelos GPT-2 para generación de poesía y texto en español","metadata":{}},{"cell_type":"markdown","source":"# Integrantes:\n\n* Yesid Castelblanco\n* Albin Rivera","metadata":{}},{"cell_type":"markdown","source":"# Introduccion","metadata":{}},{"cell_type":"markdown","source":"El presente informe documenta la implementación de un modelo de generación de texto en español, desarrollado y ejecutado en un entorno Kaggle Notebook. El objetivo principal es explorar las capacidades de los modelos de lenguaje de última generación para producir texto creativo y coherente, en este caso con énfasis en composiciones poéticas. La metodología aplicada incluye la preparación del entorno de trabajo, la instalación de librerías con versiones específicas, la carga y limpieza del conjunto de datos, la tokenización del corpus, el ajuste del modelo y la validación de los resultados a través de ejemplos generados. Para este ejercicio se utilizó el Spanish Poetry Dataset, disponible en la plataforma Hugging Face y recopilado en el marco del Somos NLP Hackathon 2022. Este conjunto de datos contiene una amplia colección de poemas en idioma español que abarca diferentes autores, estilos y épocas. ","metadata":{}},{"cell_type":"markdown","source":"# Dependencias y entorno","metadata":{}},{"cell_type":"markdown","source":"A continuación, se lleva a cabo la configuración inicial del entorno de trabajo en Kaggle. En él se definen las dependencias necesarias y se garantiza la reproducibilidad de la ejecución. Primero, se implementa una función auxiliar que permite instalar paquetes con versiones específicas, lo que asegura estabilidad y compatibilidad entre librerías. A través de esta función se instalan las bibliotecas clave para el proyecto como transformers, datasets y accelerate. Asimismo, se desinstala peft para prevenir conflictos en el entorno. Luego, se importan módulos generales de Python y librerías propias de aprendizaje profundo como torch, datasets y transformers, empleadas para el manejo de datos y la construcción del modelo de lenguaje. Posteriormente, se configura un directorio local de caché en la ruta de trabajo de Kaggle (/kaggle/working/hf_cache), con el fin de almacenar los recursos de Hugging Face y facilitar ejecuciones posteriores sin necesidad de descargas repetidas. Finalmente, se incorpora una verificación del hardware disponible para determinar si el entrenamiento podrá realizarse en GPU, mostrando en caso afirmativo el nombre de la tarjeta gráfica detectada. ","metadata":{}},{"cell_type":"code","source":"# ============================\n# 1) DEPENDENCIAS Y ENTORNO\n# ============================\nimport os, sys, subprocess\n\ndef install_package(package):\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\", \"--no-warn-conflicts\"])\n\n# Instalar dependencias clave\ninstall_package(\"transformers==4.44.2\")\ninstall_package(\"datasets==3.0.1\")\ninstall_package(\"accelerate==0.30.1\")\n\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"peft\"])\n\n# Imports base\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM,\n    Trainer, TrainingArguments,\n    DataCollatorForLanguageModeling\n)\n\n# Configuración de caché en Kaggle\ncache_dir = \"/kaggle/working/hf_cache\"\nos.makedirs(cache_dir, exist_ok=True)\nos.environ[\"HF_HOME\"] = cache_dir\n\n# Verificar GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Ejecutando en:\", device)\nif device == \"cuda\":\n    print(\"GPU:\", torch.cuda.get_device_name(0))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T20:35:17.483467Z","iopub.execute_input":"2025-09-21T20:35:17.483707Z","iopub.status.idle":"2025-09-21T20:37:15.285432Z","shell.execute_reply.started":"2025-09-21T20:35:17.483688Z","shell.execute_reply":"2025-09-21T20:37:15.284742Z"}},"outputs":[{"name":"stdout","text":"     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.7/43.7 kB 1.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.5/9.5 MB 20.3 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 38.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.6/471.6 kB 5.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.6/177.6 kB 12.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.6/302.6 kB 3.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 84.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 63.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 40.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 1.8 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 7.9 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 28.3 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 13.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 8.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 68.3 MB/s eta 0:00:00\nFound existing installation: peft 0.15.2\nUninstalling peft-0.15.2:\n  Successfully uninstalled peft-0.15.2\n","output_type":"stream"},{"name":"stderr","text":"2025-09-21 20:37:01.546149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758487021.734399      60 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758487021.799181      60 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Ejecutando en: cuda\nGPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Lo anterior indica que la instalación de las dependencias clave se realizó de manera exitosa, fijando las versiones de transformers, datasets y accelerate, y desinstalando correctamente el paquete peft para evitar conflictos. Asimismo, la verificación de hardware confirmó que la ejecución se llevará a cabo en una GPU Tesla P100-PCIE-16GB, lo cual asegura la disponibilidad de aceleración por hardware para las etapas de entrenamiento y generación de texto.","metadata":{}},{"cell_type":"markdown","source":"# Inicialización del tokenizer y modelo con configuración de special tokens","metadata":{}},{"cell_type":"markdown","source":"Se define la función init_model_and_tokenizer, cuyo propósito es inicializar el modelo de lenguaje y su tokenizador, garantizando que ambos queden configurados de manera consistente para el entrenamiento y la generación de texto. En primer lugar, la función carga el tokenizador y el modelo especificado a partir de la librería Hugging Face y los ubica en el dispositivo de cómputo disponible (CPU o GPU). Posteriormente, se asegura de que los tokens especiales estén correctamente definidos, es decir si alguno de los tokens de padding (pad), inicio (bos) o fin de secuencia (eos) no existe, se asigna un valor por defecto para evitar errores durante el proceso de entrenamiento o de inferencia. Una vez establecidos, estos identificadores se sincronizan dentro de la configuración del modelo, alineando la codificación del tokenizador con la arquitectura de la red neuronal. Finalmente, en caso de que se hayan agregado nuevos tokens especiales, se redimensionan las capas de embeddings del modelo para que correspondan al tamaño actualizado del vocabulario. La función imprime en pantalla los tokens configurados y retorna tanto el modelo como el tokenizador listos para ser utilizados en las siguientes etapas del flujo de trabajo.","metadata":{}},{"cell_type":"code","source":"def init_model_and_tokenizer(model_name, device=\"cuda\"):\n    \n    # 1. Cargar tokenizer y modelo\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n    # 2. Asegurar tokens especiales\n    if tokenizer.pad_token is None:\n        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n    if tokenizer.bos_token is None:\n        tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n    if tokenizer.eos_token is None:\n        tokenizer.add_special_tokens({\"eos_token\": \"\"})  # fallback\n\n    # 3. Alinear configuración del modelo\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model.config.eos_token_id = tokenizer.eos_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n\n    # 4. Redimensionar embeddings si se añadieron nuevos tokens\n    model.resize_token_embeddings(len(tokenizer))\n\n    print(\"✅ Modelo y tokenizer listos\")\n    print(\"pad_token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n    print(\"bos_token:\", tokenizer.bos_token, tokenizer.bos_token_id)\n    print(\"eos_token:\", tokenizer.eos_token, tokenizer.eos_token_id)\n\n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T20:37:15.286486Z","iopub.execute_input":"2025-09-21T20:37:15.287008Z","iopub.status.idle":"2025-09-21T20:37:15.292894Z","shell.execute_reply.started":"2025-09-21T20:37:15.286989Z","shell.execute_reply":"2025-09-21T20:37:15.292134Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"A continuación, se lleva a cabo la carga y depuración del conjunto de datos de poesía en español utilizado para entrenar el modelo. En primer lugar, se accede al dataset Spanish Poetry Dataset desde Hugging Face mediante la función load_dataset, especificando la ruta de caché previamente configurada para optimizar la descarga y almacenamiento local en Kaggle. Posteriormente, se realiza una verificación para identificar registros nulos o vacíos en la columna principal (content), que contiene los poemas. Una vez identificados, se filtran los textos inválidos con el fin de conservar únicamente aquellos que contienen información relevante.\n\nTras este filtrado inicial, se imprime en consola el tamaño del dataset antes y después de la limpieza, lo que permite constatar la reducción en el número de registros y la mejora en la calidad del corpus. Como medida adicional de validación, los datos se convierten a un DataFrame de pandas, donde se eliminan de forma explícita los valores nulos y las cadenas vacías que pudieran permanecer. Finalmente, el dataset limpio se reconstruye en un formato compatible con Hugging Face (DatasetDict), quedando disponible para las siguientes etapas de tokenización y preparación del entrenamiento.","metadata":{}},{"cell_type":"code","source":"# ============================\n# 2) CARGAR DATASET DE POESÍA\n# ============================\ndataset = load_dataset(\"somosnlp-hackathon-2022/spanish-poetry-dataset\", cache_dir=cache_dir)\n\n# Verificar cuántos textos nulos o vacíos hay en 'content'\nmask = [text is None or text.strip()==\"\" for text in dataset[\"train\"][\"content\"]]\nprint(\"Nulos o vacíos:\", sum(mask), \"de\", len(mask))\n\n# Filtrar solo filas con contenido válido\nds_clean = dataset[\"train\"].filter(lambda x: x[\"content\"] is not None and x[\"content\"].strip() != \"\")\n\nprint(f\"Antes: {len(dataset['train'])}, Después filtrado: {len(ds_clean)}\")\n\n# Convertir a pandas para asegurarse de eliminar restos de NA\ndf_clean = ds_clean.to_pandas().dropna(subset=[\"content\"])\ndf_clean = df_clean[df_clean[\"content\"].str.strip() != \"\"]\n\n# Reconstruir dataset HuggingFace\ndataset = DatasetDict({\n    \"train\": Dataset.from_pandas(df_clean, preserve_index=False)\n})\n\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T20:37:15.293615Z","iopub.execute_input":"2025-09-21T20:37:15.293814Z","iopub.status.idle":"2025-09-21T20:37:19.219949Z","shell.execute_reply.started":"2025-09-21T20:37:15.293794Z","shell.execute_reply":"2025-09-21T20:37:19.219297Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/196 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0265f6818c7b4851a9e83a85f5dee899"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"poems.csv: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d319e2a9e5874fa58e77dff48a4c3351"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5133 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632185fe09524bc9856a8a6a484cb022"}},"metadata":{}},{"name":"stdout","text":"Nulos o vacíos: 6 de 5133\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5133 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8a8d90c7644c9caac26d864f2e6f39"}},"metadata":{}},{"name":"stdout","text":"Antes: 5133, Después filtrado: 5127\nDatasetDict({\n    train: Dataset({\n        features: ['author', 'content', 'title'],\n        num_rows: 5127\n    })\n})\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"La ejecución de la carga y limpieza del dataset arrojó como resultado un total de 5.133 registros iniciales, de los cuales se identificaron 6 entradas nulas en la columna de contenido. Tras aplicar los filtros correspondientes para eliminar estos registros inválidos, el conjunto de datos quedó conformado por 5.127 poemas válidos. Finalmente, el dataset se reconstruyó en formato DatasetDict de Hugging Face, manteniendo las tres variables originales siendo estas autor, contenido y título. De esta forma, el corpus depurado quedó listo para su uso en las siguientes fases de tokenización y entrenamiento del modelo de generación de texto. \n\nSe procede a transformar el dataset previamente cargado y limpiado a un formato pandas DataFrame, lo cual facilita la manipulación y el análisis exploratorio de los datos. Los resultados muestran que el dataset está compuesto por tres columnas principales siendo estas author (nombre del autor del poema), content (texto del poema) y title (título de la obra). En la salida se observan fragmentos de poemas de diversos autores, entre ellos Leopoldo Lugones, Gabriela Mistral, Antonio Colinas y William Shakespeare. Esta visualización confirma tanto la validez del corpus como la diversidad de estilos y épocas que lo conforman, evidenciando que se cuenta con un recurso literario amplio y variado para el entrenamiento del modelo de generación de texto.\n","metadata":{}},{"cell_type":"code","source":"dataset.set_format('pandas')\ndf = dataset['train'].to_pandas()\ndf.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T20:37:19.221636Z","iopub.execute_input":"2025-09-21T20:37:19.221864Z","iopub.status.idle":"2025-09-21T20:37:19.252951Z","shell.execute_reply.started":"2025-09-21T20:37:19.221846Z","shell.execute_reply":"2025-09-21T20:37:19.252396Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                    author                                            content  \\\n0         Leopoldo Lugones  \\n\\nEn el parque confuso\\nQue con lánguidas br...   \n1          Marilina Rébora  \\n\\nPorque si tú no velas, vendré como ladrón;...   \n2          Antonio Colinas  \\n\\nPequeña de mis sueños, por tu piel las pal...   \n3      José María Hinojosa  \\n\\nLos dedos de la nieve\\nrepiquetearon\\nen e...   \n4  Rubén Izaguirre Fiallos  Naciste en Armenia,\\npero te fuiste a vivir al...   \n5    Leopoldo María Panero  \\n\\nOscuridad nieve buitres desespero oscurida...   \n6         Gabriela Mistral  \\nSiento mi corazón en la dulzura \\nfundirse c...   \n7             Pablo Neruda  Cien sonetos de amor\\n\\nTrajo el amor su cola ...   \n8      William Shakespeare  ¿Y por qué no es tu guerra más pujante\\ncontra...   \n9         Gabriela Mistral  \\nEl espino prende a una roca \\nsu enloquecida...   \n\n                                      title  \n0                      LA MUERTE DE LA LUNA  \n1                     PORQUE SI TÚ NO VELAS  \n2     POEMA DE LA BELLEZA CAUTIVA QUE PERDÍ  \n3                                 SENCILLEZ  \n4             Breve Carta a Consuelo Suncín  \n5                          PASADIZO SECRETO  \n6                                 Atardecer  \n7                      Cien sonetos de amor  \n8  Y por qué no es tu guerra más pujante...  \n9                                 El espino  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>content</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Leopoldo Lugones</td>\n      <td>\\n\\nEn el parque confuso\\nQue con lánguidas br...</td>\n      <td>LA MUERTE DE LA LUNA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Marilina Rébora</td>\n      <td>\\n\\nPorque si tú no velas, vendré como ladrón;...</td>\n      <td>PORQUE SI TÚ NO VELAS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Antonio Colinas</td>\n      <td>\\n\\nPequeña de mis sueños, por tu piel las pal...</td>\n      <td>POEMA DE LA BELLEZA CAUTIVA QUE PERDÍ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>José María Hinojosa</td>\n      <td>\\n\\nLos dedos de la nieve\\nrepiquetearon\\nen e...</td>\n      <td>SENCILLEZ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Rubén Izaguirre Fiallos</td>\n      <td>Naciste en Armenia,\\npero te fuiste a vivir al...</td>\n      <td>Breve Carta a Consuelo Suncín</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Leopoldo María Panero</td>\n      <td>\\n\\nOscuridad nieve buitres desespero oscurida...</td>\n      <td>PASADIZO SECRETO</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Gabriela Mistral</td>\n      <td>\\nSiento mi corazón en la dulzura \\nfundirse c...</td>\n      <td>Atardecer</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Pablo Neruda</td>\n      <td>Cien sonetos de amor\\n\\nTrajo el amor su cola ...</td>\n      <td>Cien sonetos de amor</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>William Shakespeare</td>\n      <td>¿Y por qué no es tu guerra más pujante\\ncontra...</td>\n      <td>Y por qué no es tu guerra más pujante...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Gabriela Mistral</td>\n      <td>\\nEl espino prende a una roca \\nsu enloquecida...</td>\n      <td>El espino</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"A continuación,se calcula la extensión de cada poema en número de palabras para caracterizar el corpus de manera cuantitativa. Para ello, se crea la columna Palabras_por_poesia, donde cada valor corresponde al conteo de palabras en el texto del campo content. Una vez generado este indicador, se obtiene la mediana de longitud de los poemas, la cual resultó en 107 palabras por composición. Este valor sirve como referencia estadística sobre la extensión típica de los textos, información clave para definir parámetros posteriores en la tokenización y entrenamiento del modelo.","metadata":{}},{"cell_type":"code","source":"df['Palabras_por_poesia'] = (\n    df['content']\n    .fillna(\"\")\n    .str.split()\n    .apply(len)\n)\n\nprint(f\"La mediana de palabras por poesía es: {df['Palabras_por_poesia'].median()} palabras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T20:37:19.253621Z","iopub.execute_input":"2025-09-21T20:37:19.253805Z","iopub.status.idle":"2025-09-21T20:37:19.402492Z","shell.execute_reply.started":"2025-09-21T20:37:19.253790Z","shell.execute_reply":"2025-09-21T20:37:19.401800Z"}},"outputs":[{"name":"stdout","text":"La mediana de palabras por poesía es: 107.0 palabras\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Ya con lo anterior se procede a ejecutar la tokenización y partición del dataset para preparar los datos para el entrenamiento del modelo. Primero, se define la función preprocess_function, la cual utiliza el tokenizador previamente configurado para transformar los poemas en secuencias de identificadores numéricos (input_ids). En este paso, se establece una longitud máxima de secuencia de 256 tokens, aplicando truncamiento en los textos más largos y padding en los más cortos, con el fin de homogeneizar las entradas. Luego, se aplica esta función a todo el dataset con .map(), obteniendo como salida un conjunto tokenizado en el que se conservan únicamente las columnas necesarias. Posteriormente, se realiza una división en subconjuntos de entrenamiento y prueba, destinando el 90% de los datos al entrenamiento y el 10% restante a la evaluación. Finalmente, el dataset se configura en formato torch para que pueda ser utilizado directamente por el framework PyTorch durante el entrenamiento.","metadata":{}},{"cell_type":"code","source":"def preprocess_function(max_len):\n    def _preprocess_function(examples):\n        return tokenizer(examples['content'], max_length=max_len, truncation=True, padding='max_length')\n    return _preprocess_function\nmodel_name = \"datificate/gpt2-small-spanish\"\nmodel, tokenizer = init_model_and_tokenizer(model_name, device=device)\ndataset.reset_format()\ntokenized_dataset = dataset['train'].map(preprocess_function(max_len=256), batched=True)\ntokenized_dataset = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col != 'input_ids'])\ntokenized_dataset = tokenized_dataset.train_test_split(train_size=0.9)\ntokenized_dataset.set_format('torch')\ntokenized_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T20:37:19.403238Z","iopub.execute_input":"2025-09-21T20:37:19.403526Z","iopub.status.idle":"2025-09-21T20:37:27.869603Z","shell.execute_reply.started":"2025-09-21T20:37:19.403499Z","shell.execute_reply":"2025-09-21T20:37:27.869018Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fce4e2e09f4d454daf48b5eeb04b0155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/817 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1383fd434c0f48f3b32f1441af7e1393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"330a7b57fb764a438e69b7ec5657a72c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54cc4d5f834c46f0b36cebc4b8b364fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdbd89b6ca9d48b98644623eb3f3e18f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7ceeec1bd994a21bcce43b1da5acb75"}},"metadata":{}},{"name":"stdout","text":"✅ Modelo y tokenizer listos\npad_token: <|endoftext|> 0\nbos_token: <|endoftext|> 0\neos_token: <|endoftext|> 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5127 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb7e82b4845d415081904a70e2f3b63f"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids'],\n        num_rows: 4614\n    })\n    test: Dataset({\n        features: ['input_ids'],\n        num_rows: 513\n    })\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"Los resultados muestran que, de un total de 5.127 poemas válidos, se asignaron 4.614 registros al conjunto de entrenamiento y 513 al de prueba, ambos conteniendo únicamente la representación numérica de los textos en la forma de la columna input_ids. ","metadata":{}},{"cell_type":"markdown","source":"# Validacion rapida para saber a que modelo darle mas tiempo","metadata":{}},{"cell_type":"markdown","source":"Se procede a realizar una evaluación comparativa de distintos modelos de lenguaje en español y en inglés adaptados al castellano, entrenándolos de manera rápida sobre un subconjunto reducido de datos. Para cada modelo se registró el valor de loss, el tiempo de entrenamiento y un ejemplo de texto generado a partir del prompt “En el silencio de la noche”. Los resultados se organizaron en una tabla que facilitó la comparación entre las alternativas, y a partir de este análisis se seleccionó automáticamente el modelo con menor. De esta manera se identifico que el modelo que arrojo mejor desempeño y capacidad generativa en la validación fue datificate/gpt2-small-spanish\", \"gpt2-small-es.","metadata":{}},{"cell_type":"code","source":"\nimport time\nimport pandas as pd\nfrom transformers import (\n    AutoModelForCausalLM, AutoTokenizer,\n    Trainer, TrainingArguments, DataCollatorForLanguageModeling\n)\n\n\n\n# ============================\n# FUNCIÓN DE EVALUACIÓN\n# ============================\ndef evaluar_modelo(model_name, dataset, nombre=None, epochs=1, batch_size=4):\n    \n    print(f\"\\n=== Entrenando {nombre or model_name} ===\")\n\n    # Tokenizador y modelo\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        \n    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n    # Argumentos de entrenamiento\n    training_args = TrainingArguments(\n        output_dir=f\"./{nombre}-out\",\n        overwrite_output_dir=True,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        evaluation_strategy=\"steps\",\n        eval_steps=100,\n        logging_steps=50,\n        save_strategy=\"no\",        # no guardamos checkpoints en pruebas\n        report_to=\"none\",\n        disable_tqdm=True,\n        fp16=True\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n        train_dataset=dataset[\"train\"].select(range(500)),  # subset para prueba rápida\n        eval_dataset=dataset[\"test\"].select(range(100)),\n        tokenizer=tokenizer\n    )\n\n    start = time.time()\n    result = trainer.train()\n    end = time.time()\n\n    tiempo = end - start\n    loss = result.training_loss\n\n    # Generación de ejemplo corto\n    prompt = \"En el silencio de la noche\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(\n        **inputs,\n        max_length=60,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95\n    )\n    ejemplo = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return {\n        \"Modelo\": nombre or model_name,\n        \"Loss\": round(loss, 4),\n        \"Tiempo (seg)\": round(tiempo, 2),\n        \"Ejemplo\": ejemplo[:200]  # recortamos\n    }\n\n# ============================\n# LISTA DE MODELOS A PROBAR\n# ============================\nmodelos = [\n    (\"distilgpt2\", \"distilgpt2\"),\n    (\"flax-community/gpt-2-spanish\", \"gpt-2-spanish\"),\n    (\"mrm8488/spanish-gpt2\", \"spanish-gpt2\"),\n    (\"datificate/gpt2-small-spanish\", \"gpt2-small-es\"),\n    (\"ITG/DialoGPT-medium-spanish-chitchat\", \"DialoGPT-medium-spanish-chitchat\"),\n    (\"ostorc/Conversational_Spanish_GPT\", \"Conversational_Spanish_GPT\"),\n    (\"EleutherAI/gpt-neo-125M\", \"gpt-neo-125M\")\n]\n\n\n# ============================\n# EJECUTAR EXPERIMENTOS\n# ============================\nresultados = []\nfor model_id, nombre in modelos:\n    res = evaluar_modelo(model_id, tokenized_dataset, nombre=nombre, epochs=1, batch_size=4)\n    resultados.append(res)\n\n# Mostrar resultados\ndf_resultados = pd.DataFrame(resultados)\nprint(df_resultados)\n\n# ============================\n# SELECCIÓN AUTOMÁTICA DEL GANADOR\n# ============================\nganador = df_resultados.sort_values(\"Loss\").iloc[0]\nprint(\"\\n=== MODELO RECOMENDADO ===\")\nprint(f\"Modelo: {ganador['Modelo']} | Loss: {ganador['Loss']} | Tiempo: {ganador['Tiempo (seg)']} seg\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T20:37:27.870223Z","iopub.execute_input":"2025-09-21T20:37:27.870454Z","iopub.status.idle":"2025-09-21T20:41:51.821528Z","shell.execute_reply.started":"2025-09-21T20:37:27.870437Z","shell.execute_reply":"2025-09-21T20:41:51.820671Z"}},"outputs":[{"name":"stdout","text":"\n=== Entrenando distilgpt2 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d695c8934824c699e7fe0677de976d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e514b769f7274963a1d2389d21dc2319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"546d6738af3541b28cf4348901482e80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e41d0dbd865142c28be62eb99b0a501c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a42855c8114d42b88f85d58b145ae335"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e54cc247f3044f498f38afde3c71cc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab85bc63e32649b49de50a0a13586c7e"}},"metadata":{}},{"name":"stdout","text":"{'loss': 5.819, 'grad_norm': 10.013121604919434, 'learning_rate': 3.16e-05, 'epoch': 0.4}\n{'loss': 5.0841, 'grad_norm': 15.786567687988281, 'learning_rate': 1.2e-05, 'epoch': 0.8}\n{'eval_loss': 4.548590660095215, 'eval_runtime': 0.8553, 'eval_samples_per_second': 116.912, 'eval_steps_per_second': 29.228, 'epoch': 0.8}\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 15.9662, 'train_samples_per_second': 31.316, 'train_steps_per_second': 7.829, 'train_loss': 5.360367614746094, 'epoch': 1.0}\n\n=== Entrenando gpt-2-spanish ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38ce52117a46481c91c8e54f9fc8b36a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bef4d8c93d504fd2b2c5bf112821af39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96042ba9acb4fdab1513aa276cf87a7"}},"metadata":{}},{"name":"stdout","text":"{'loss': 5.8653, 'grad_norm': 2.5029141902923584, 'learning_rate': 3e-05, 'epoch': 0.4}\n{'loss': 5.4582, 'grad_norm': 1.9385299682617188, 'learning_rate': 1e-05, 'epoch': 0.8}\n{'eval_loss': 5.106327056884766, 'eval_runtime': 1.3128, 'eval_samples_per_second': 76.171, 'eval_steps_per_second': 19.043, 'epoch': 0.8}\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 23.1053, 'train_samples_per_second': 21.64, 'train_steps_per_second': 5.41, 'train_loss': 5.625998168945313, 'epoch': 1.0}\n\n=== Entrenando spanish-gpt2 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/226 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b274229450d45b6a0af32ef34489d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c0ab9ecf1144512b4a0f9be1bf5a0d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff71b86e433464f8a898301442b03c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf4f2d8e2b2449829b0219320151d843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7b431707c9432eab284ba0e1f44a0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7310bd188064dbf80d7dc8f74a5dc26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/883 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11fded77a92844b08438a9917fdc68c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f27dc9e79d5428195fa4b5dde4ab230"}},"metadata":{}},{"name":"stdout","text":"{'loss': 6.6573, 'grad_norm': 1.464296579360962, 'learning_rate': 3.08e-05, 'epoch': 0.4}\n{'loss': 5.7455, 'grad_norm': 1.311340093612671, 'learning_rate': 1.08e-05, 'epoch': 0.8}\n{'eval_loss': 5.335311412811279, 'eval_runtime': 1.3119, 'eval_samples_per_second': 76.223, 'eval_steps_per_second': 19.056, 'epoch': 0.8}\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 23.0694, 'train_samples_per_second': 21.674, 'train_steps_per_second': 5.418, 'train_loss': 6.104834716796875, 'epoch': 1.0}\n\n=== Entrenando gpt2-small-es ===\n{'loss': 4.5346, 'grad_norm': 3.574758291244507, 'learning_rate': 3e-05, 'epoch': 0.4}\n{'loss': 4.3743, 'grad_norm': 3.7373454570770264, 'learning_rate': 1e-05, 'epoch': 0.8}\n{'eval_loss': 4.1871514320373535, 'eval_runtime': 1.3134, 'eval_samples_per_second': 76.136, 'eval_steps_per_second': 19.034, 'epoch': 0.8}\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 23.3002, 'train_samples_per_second': 21.459, 'train_steps_per_second': 5.365, 'train_loss': 4.4528349609375, 'epoch': 1.0}\n\n=== Entrenando DialoGPT-medium-spanish-chitchat ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/928 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1613e5176e5943dabacee3ebc9d61eb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e886dd09d9a344e398f0f90711b1b1d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c305762a7834f6a8338618872931a85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de9aa27743f14bf0bc0834bc52bccf21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ddb917a8a414604b80b6b0cfe52e1e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/815 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63edcfc0b9454a94882af9c0d6498b25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fd80dfdc71948ee84b049af732e2be1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab203ca8160c49b8893092afc359fdaa"}},"metadata":{}},{"name":"stdout","text":"{'loss': 6.6473, 'grad_norm': 2.948674201965332, 'learning_rate': 3.16e-05, 'epoch': 0.4}\n{'loss': 5.2585, 'grad_norm': 3.769627571105957, 'learning_rate': 1.16e-05, 'epoch': 0.8}\n{'eval_loss': 4.727499961853027, 'eval_runtime': 3.4006, 'eval_samples_per_second': 29.407, 'eval_steps_per_second': 7.352, 'epoch': 0.8}\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 61.9558, 'train_samples_per_second': 8.07, 'train_steps_per_second': 2.018, 'train_loss': 5.790750122070312, 'epoch': 1.0}\n\n=== Entrenando Conversational_Spanish_GPT ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/514 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de812ecfea2644b9b9f10e40be07e88b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ca3476fd104ca395ac6d4b7f9a48de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b104cc3adb65476fac0f5c5505f4baff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b442132cf7b5437aab6ae3ce85da281b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f3ba2e71ab241c58e261aa48e844f41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/903 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b74f5a158fcc461588e7e51caa286384"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7e1e57e78354561a871679cc456d333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67022333e2424582becca0734d60e158"}},"metadata":{}},{"name":"stdout","text":"{'loss': 8.6633, 'grad_norm': 5.966269016265869, 'learning_rate': 3.16e-05, 'epoch': 0.4}\n{'loss': 5.6266, 'grad_norm': 8.659952163696289, 'learning_rate': 1.16e-05, 'epoch': 0.8}\n{'eval_loss': 5.016141891479492, 'eval_runtime': 1.3256, 'eval_samples_per_second': 75.438, 'eval_steps_per_second': 18.86, 'epoch': 0.8}\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 23.3453, 'train_samples_per_second': 21.418, 'train_steps_per_second': 5.354, 'train_loss': 6.80870166015625, 'epoch': 1.0}\n\n=== Entrenando gpt-neo-125M ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d285c5bbfdc4f3f8db06b9dcd15f899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f8329c41e384038910f7e4f742145cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7446915379644732990b9a344451765d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3810a90e8c5442c9bc037a00fa85ab7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d03761344640c7af6f3289aa741130"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f5fd1ee75e44165999602e9a90b6650"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b80b9bc32843119cffa8a2464719f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ffc7f2d6f7240bbb57c3d2b91f9807e"}},"metadata":{}},{"name":"stdout","text":"{'loss': 5.0299, 'grad_norm': 2.475013256072998, 'learning_rate': 3.04e-05, 'epoch': 0.4}\n{'loss': 4.5348, 'grad_norm': 3.2140684127807617, 'learning_rate': 1.04e-05, 'epoch': 0.8}\n{'eval_loss': 4.191422462463379, 'eval_runtime': 1.4913, 'eval_samples_per_second': 67.058, 'eval_steps_per_second': 16.764, 'epoch': 0.8}\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 24.9286, 'train_samples_per_second': 20.057, 'train_steps_per_second': 5.014, 'train_loss': 4.738563232421875, 'epoch': 1.0}\n                             Modelo    Loss  Tiempo (seg)  \\\n0                        distilgpt2  5.3604         16.37   \n1                     gpt-2-spanish  5.6260         23.55   \n2                      spanish-gpt2  6.1048         23.56   \n3                     gpt2-small-es  4.4528         23.79   \n4  DialoGPT-medium-spanish-chitchat  5.7908         62.45   \n5        Conversational_Spanish_GPT  6.8087         24.02   \n6                      gpt-neo-125M  4.7386         25.42   \n\n                                             Ejemplo  \n0  En el silencio de la noche to o l- a a to u o ...  \n1  En el silencio de la noche, la voz de la noche...  \n2  En el silencio de la nocheLa música de amor qu...  \n3  En el silencio de la noche, una mujer lloraba ...  \n4  En el silencio de la noche pregunta toc tocahe...  \n5                 En el silencio de la noche de todo  \n6  En el silencio de la noche con tolenthe coment...  \n\n=== MODELO RECOMENDADO ===\nModelo: gpt2-small-es | Loss: 4.4528 | Tiempo: 23.79 seg\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Entrenamiento del modelo con un solo dataset","metadata":{}},{"cell_type":"markdown","source":"Ya con la selección del modelo, se llevó a cabo el entrenamiento, utilizando el corpus de poesía previamente tokenizado. Para ello, se configuraron los parámetros de ajuste fino con 20 épocas de entrenamiento, batch size de 32, tasa de aprendizaje de 3e-5 y estrategias de evaluación y guardado cada 200 pasos. Se incluyó un callback personalizado que calculó y mostró la perplejidad (PPL) junto con la pérdida de validación, lo que permitió monitorear en tiempo real la capacidad del modelo para predecir secuencias de palabras. Con esta configuración, el proceso de entrenamiento optimizó de manera progresiva los parámetros del modelo, garantizando un mejor ajuste al estilo y estructura propios de los poemas en español.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainerCallback\nimport math\n\n# ============================\n# CALLBACK PERSONALIZADO\n# ============================\nclass PerplexityCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        if metrics and \"eval_loss\" in metrics:\n            ppl = math.exp(metrics[\"eval_loss\"])\n            print(f\"\\n🔎 Step {state.global_step}: Eval Loss = {metrics['eval_loss']:.4f} | PPL = {ppl:.2f}\")\n\n# ============================\n# 3) TOKENIZADOR Y MODELO BASE\n# ============================\nmodel_name = \"datificate/gpt2-small-spanish\"\nmodel, tokenizer = init_model_and_tokenizer(model_name, device=device)\n\nbatch_size = 32\n\ntraining_args = TrainingArguments(\n    output_dir=\"./hf-gpt\",\n    run_name=\"poesia-gpt2\",\n    overwrite_output_dir=True,\n    num_train_epochs=20,#20\n    learning_rate=3e-5,#3e-5, 5e-5\n    per_device_eval_batch_size=batch_size,\n    per_device_train_batch_size=batch_size,\n    weight_decay=0.05,#0.01\n    warmup_steps=40, #1500, 100\n    evaluation_strategy=\"steps\",  \n    eval_steps=200,               \n    save_steps=200,               \n    logging_strategy=\"steps\",\n    logging_steps=50,\n    logging_first_step=True,\n    save_strategy=\"steps\",    \n    save_total_limit=2,\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    disable_tqdm=False,\n    fp16=True,\n    gradient_accumulation_steps = 2 #Nuevo\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    callbacks=[PerplexityCallback]   # 👈 Agregamos el callback\n)\n\n# ============================\n# 4) ENTRENAMIENTO\n# ============================\ntrainer.train()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T20:41:51.822458Z","iopub.execute_input":"2025-09-21T20:41:51.822741Z","iopub.status.idle":"2025-09-21T21:35:01.530277Z","shell.execute_reply.started":"2025-09-21T20:41:51.822716Z","shell.execute_reply":"2025-09-21T21:35:01.529681Z"}},"outputs":[{"name":"stdout","text":"✅ Modelo y tokenizer listos\npad_token: <|endoftext|> 0\nbos_token: <|endoftext|> 0\neos_token: <|endoftext|> 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1440/1440 53:05, Epoch 19/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>4.086900</td>\n      <td>3.988784</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.917300</td>\n      <td>3.926177</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.836800</td>\n      <td>3.898781</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.772700</td>\n      <td>3.882641</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.731300</td>\n      <td>3.871307</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.702900</td>\n      <td>3.867567</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.667100</td>\n      <td>3.864728</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n🔎 Step 200: Eval Loss = 3.9888 | PPL = 53.99\n\n🔎 Step 400: Eval Loss = 3.9262 | PPL = 50.71\n\n🔎 Step 600: Eval Loss = 3.8988 | PPL = 49.34\n\n🔎 Step 800: Eval Loss = 3.8826 | PPL = 48.55\n\n🔎 Step 1000: Eval Loss = 3.8713 | PPL = 48.01\n\n🔎 Step 1200: Eval Loss = 3.8676 | PPL = 47.83\n\n🔎 Step 1400: Eval Loss = 3.8647 | PPL = 47.69\n","output_type":"stream"},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1440, training_loss=3.855002742343479, metrics={'train_runtime': 3187.988, 'train_samples_per_second': 28.946, 'train_steps_per_second': 0.452, 'total_flos': 1.1975797702656e+16, 'train_loss': 3.855002742343479, 'epoch': 19.862068965517242})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def generar_texto(model, tokenizer, prompt=\"En el silencio de la noche\"):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(\n        **inputs,\n        max_length=120,\n        num_return_sequences=5,\n        temperature=1.1,\n        top_k=30,\n        top_p=0.95,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id   # 🔑 ya definido arriba\n    )\n    for i, out in enumerate(outputs):\n        print(f\"=== Poema {i+1} ===\")\n        print(tokenizer.decode(out, skip_special_tokens=True))\n        print()\n\ngenerar_texto(model, tokenizer, \"Había una vez un poeta que soñaba\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T21:35:01.531019Z","iopub.execute_input":"2025-09-21T21:35:01.531203Z","iopub.status.idle":"2025-09-21T21:35:03.071562Z","shell.execute_reply.started":"2025-09-21T21:35:01.531188Z","shell.execute_reply":"2025-09-21T21:35:03.070693Z"}},"outputs":[{"name":"stdout","text":"=== Poema 1 ===\nHabía una vez un poeta que soñaba\nuna enagua, y otra en las esquinas, o una \nsutura que tenía un sabor a tu boca. \nEra la primavera, que era de un día\ny aún eran verdes, pero eran verdes\npara mí mismos. \nY por esta razón no recuerdo el poeta\nque no entendía. \nEn las tardes de los días\ncon un sol y el mar con un rayo, \no la vida de los pájaros o en los árboles \no la soledad del aire o el agua con lluvia. \nNo\n\n=== Poema 2 ===\nHabía una vez un poeta que soñaba con\nla noche de verano, \ny en sus libros\nvuelto en recuerdos, \ny de la mañana, \nen los libros que de noche\nse habían ido. \nSu verso era la tarde y la madrugada; \nel ritmo del viento se volvía \ny las olas que las aguas arrastraban. \nSus versos eran la noche\nnuestros poemas, y todo su trabajo \nse pasaba. \nA la noche del día se leía: \nloco de mí y la hora. \n\n\n=== Poema 3 ===\nHabía una vez un poeta que soñaba de un amor que no le fue concedida: un poeta de amor.  \nLos poetas de mi región no le sontados, ni las glorias de los tiempos modernos,      los de otros tiempos.\nY la poesía que no tiene un destino más que el de su padre. \nEl poeta de un deseo\nde hacer más versos que amor que nunca he querido.  \nEs la poesía que se ha perdido y no se ha vuelto. \nY es de mi amor que hay una poesía de amor. \n\n=== Poema 4 ===\nHabía una vez un poeta que soñaba con el mundo. \n               El poeta era un poeta que soñaba con el mundo. \n                                                       El poeta era una luz de cielo y un cielo profundo. \n             \n\n=== Poema 5 ===\nHabía una vez un poeta que soñaba\ncuando lo escuchó,\nme dijo «no es mío» como dice su hijo.\nEra un niño muy infantil:\nsu padre era un obrero;\ntenía un hijo de un ingeniero;\nbatallaba la profesión de la carpintería;\nno le llamaban «yo» y «yo» porque era menor que yo,\nde carácter y con un traje.\nEra muy rico y de buen carácter:\nle decía al fin que le llamaba siempre porque era menor.\nEl padre de Bodijay no era poeta;\nera un\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"El entrenamiento del modelo gpt2-small-spanish alcanzó valores finales de training loss cercanos a 3.67 y un validation loss de 3.86, lo que se tradujo en una perplejidad (PPL) de 47.69. Posteriormente, se evaluó la capacidad creativa del modelo mediante la función de generación de texto, utilizando como prompt inicial “Había una vez un poeta que soñaba”. Los resultados muestran cinco poemas producidos de manera automática, en los cuales el modelo logró mantener una estructura literaria coherente, evocando temáticas propias de la poesía como el amor, la vida, los recuerdos y la naturaleza; sin embargo, también se evidencian repeticiones y pasajes con baja coherencia semántica, reflejo de las limitaciones del entrenamiento con un corpus reducido. Ante estos resultados, se tomó la decisión de combinar el dataset de poesía con otros conjuntos de datos, con el fin de enriquecer el vocabulario, mejorar la capacidad de generalización y reducir la perplejidad en futuras iteraciones.","metadata":{}},{"cell_type":"markdown","source":"# Limpieza de almacenamiento y de memoria para la ejecucion de la combinacion de datasets","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\nimport gc\nimport torch\n\n# ============================\n# BORRAR CARPETAS EN DISCO\n# ============================\nfolders = [\n    \"/kaggle/working/Conversational_Spanish_GPT-out\",\n    \"/kaggle/working/DialoGPT-medium-spanish-chitchat-out\",\n    \"/kaggle/working/distilgpt2-out\",\n    \"/kaggle/working/gpt-2-spanish-out\",\n    \"/kaggle/working/gpt-neo-125M-out\",\n    \"/kaggle/working/gpt2-small-es-out\",\n    \"/kaggle/working/hf-gpt\",\n    \"/kaggle/working/hf_cache\", \n    \"/kaggle/working/spanish-gpt2-out\"\n]\n\nfor f in folders:\n    if os.path.exists(f):\n        shutil.rmtree(f, ignore_errors=True)\n        print(f\"🗑️ Borrado: {f}\")\n    else:\n        print(f\"⚠️ No existe: {f}\")\n\n# ============================\n# LIBERAR RAM Y VRAM\n# ============================\ngc.collect()  # limpia objetos de Python en memoria\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()  # libera VRAM no usada\n    torch.cuda.ipc_collect()  # recolecta memoria inter-procesos\n    print(\"🔋 Memoria RAM y VRAM liberadas.\")\nelse:\n    print(\"⚠️ No hay GPU disponible, solo se limpió RAM.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T21:35:03.073458Z","iopub.execute_input":"2025-09-21T21:35:03.073722Z","iopub.status.idle":"2025-09-21T21:35:04.744878Z","shell.execute_reply.started":"2025-09-21T21:35:03.073706Z","shell.execute_reply":"2025-09-21T21:35:04.744285Z"}},"outputs":[{"name":"stdout","text":"🗑️ Borrado: /kaggle/working/Conversational_Spanish_GPT-out\n🗑️ Borrado: /kaggle/working/DialoGPT-medium-spanish-chitchat-out\n🗑️ Borrado: /kaggle/working/distilgpt2-out\n🗑️ Borrado: /kaggle/working/gpt-2-spanish-out\n🗑️ Borrado: /kaggle/working/gpt-neo-125M-out\n🗑️ Borrado: /kaggle/working/gpt2-small-es-out\n🗑️ Borrado: /kaggle/working/hf-gpt\n🗑️ Borrado: /kaggle/working/hf_cache\n🗑️ Borrado: /kaggle/working/spanish-gpt2-out\n🔋 Memoria RAM y VRAM liberadas.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Combinación de  Datasets","metadata":{}},{"cell_type":"markdown","source":"Con base en el resultado anterior y la desición posterior, se lleva a cabo la carga y combinación de distintos corpus en español para fortalecer el entrenamiento del modelo de lenguaje. En primer lugar, se descargan tres conjuntos de datos desde Hugging Face, un corpus de poesía en español (Spanish Poetry Dataset), una muestra del Wikipedia en español y un subconjunto de noticias del dataset MLSUM en español. Dado que la ejecución se realiza en una GPU P100, se utilizan únicamente fracciones reducidas de Wikipedia y MLSUM para optimizar recursos. Posteriormente, se aplica un proceso de limpieza de datos, eliminando entradas vacías o nulas, y se normaliza la estructura de los tres corpus bajo una única columna llamada “content”. A continuación, cada dataset se convierte a formato pandas DataFrame, lo que permite concatenarlos en un único corpus unificado. Sobre este conjunto combinado se ejecuta una limpieza final para remover posibles valores faltantes, y luego se reconstruye en formato Dataset de Hugging Face, dividiéndolo en subconjuntos de entrenamiento (90%) y prueba (10%).\n\nEl resultado es un corpus integrado que combina poesía, textos enciclopédicos y noticias en español, aportando diversidad léxica y temática. Este enriquecimiento busca mejorar la generalización del modelo y reducir la perplejidad obtenida en entrenamientos previos con un corpus limitado únicamente a textos poéticos.","metadata":{}},{"cell_type":"code","source":"# ============================\n# 2) CARGA Y COMBINACIÓN DE CORPUS\n# ============================\nfrom datasets import load_dataset, Dataset, DatasetDict\nimport pandas as pd\n\n# Dataset de poesía\npoesias = load_dataset(\"somosnlp-hackathon-2022/spanish-poetry-dataset\", cache_dir=cache_dir)\n\n# Dataset de Wikipedia en español (usar solo una fracción para GPU P100)\nwiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.es\", split=\"train[:1%]\", cache_dir=cache_dir)\n\n# Dataset de noticias MLSUM en español (usar solo una fracción)\nnoticias = load_dataset(\"mlsum\", \"es\", split=\"train[:1%]\", trust_remote_code=True, cache_dir=cache_dir)\n\n# 🔹 Limpieza\ndef filter_valid(ex):\n    text = ex.get(\"content\", ex.get(\"text\", None))\n    return text is not None and text.strip() != \"\"\n\npoesias = poesias[\"train\"].filter(lambda x: x[\"content\"] is not None and x[\"content\"].strip() != \"\")\nwiki = wiki.filter(lambda x: x[\"text\"] is not None and x[\"text\"].strip() != \"\")\nnoticias = noticias.filter(lambda x: x[\"text\"] is not None and x[\"text\"].strip() != \"\")\n\n# 🔹 Normalizar a columna \"content\"\npoesias = poesias.map(lambda x: {\"content\": x[\"content\"]})\nwiki = wiki.map(lambda x: {\"content\": x[\"text\"]})\nnoticias = noticias.map(lambda x: {\"content\": x[\"text\"]})\n\n# 🔹 Convertir a pandas\ndf_poesias = poesias.to_pandas()[[\"content\"]]\ndf_wiki = wiki.to_pandas()[[\"content\"]]\ndf_noticias = noticias.to_pandas()[[\"content\"]]\n\n# 🔹 Concatenar y limpiar\ndf_comb = pd.concat([df_poesias, df_wiki, df_noticias], ignore_index=True)\ndf_comb = df_comb.dropna().reset_index(drop=True)\n\n# 🔹 Reconstruir Dataset HuggingFace\ndataset = Dataset.from_pandas(df_comb)\ndataset = dataset.train_test_split(train_size=0.9, seed=42)\n\ndataset = DatasetDict({\n    \"train\": dataset[\"train\"],\n    \"test\": dataset[\"test\"]\n})\n\nprint(dataset)\nprint(\"Ejemplo:\", dataset[\"train\"][0])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T21:35:04.745626Z","iopub.execute_input":"2025-09-21T21:35:04.745873Z","iopub.status.idle":"2025-09-21T21:38:39.780105Z","shell.execute_reply.started":"2025-09-21T21:35:04.745850Z","shell.execute_reply":"2025-09-21T21:38:39.779290Z"}},"outputs":[{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5133 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9963ad4c9fec4c27a114f251ea836d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27d3f6e6b5f44f9d8c2ae3ce51f74165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00000-of-00013.parquet:   0%|          | 0.00/688M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e617b1bf8ede43038d9433d84f63f125"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00001-of-00013.parquet:   0%|          | 0.00/376M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c004cb0860f47d08d64543fc915e0eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00002-of-00013.parquet:   0%|          | 0.00/287M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d7bd31a219e40d08798db92ccd27291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00003-of-00013.parquet:   0%|          | 0.00/245M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc320ef7fd8448cf8c86b07539be37b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00004-of-00013.parquet:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24bb85630759420782efec04b9c9db41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00005-of-00013.parquet:   0%|          | 0.00/178M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e89b43dc26494948a6c2c7d6a222fa27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00006-of-00013.parquet:   0%|          | 0.00/216M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"135230355bd045fdae8f2b4214b6e37e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00007-of-00013.parquet:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e89fcc8bba214253a0f5f49c67fac4b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00008-of-00013.parquet:   0%|          | 0.00/227M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f66d7c07a846cb85be74c9fb4bcd8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00009-of-00013.parquet:   0%|          | 0.00/223M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5bb1766f9c44a508e51b2732b12a11f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00010-of-00013.parquet:   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc4bd186e2a44d2e994259a68854ac50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00011-of-00013.parquet:   0%|          | 0.00/254M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae8a101e25c64626810967b529d12a07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"20231101.es/train-00012-of-00013.parquet:   0%|          | 0.00/226M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9128f0a2cdaa4b779c61329274fe49e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1841155 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b232bb5f3243549d83273cfe5a3ec7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mlsum.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a8830d7c9104f6998345ee2469ba05d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8675b82502484576a45d38b58da5e8e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.32G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27537aae51f344b1859f4443586be5d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/55.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce013e488f0847ee8e55fa6acc7beb62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/77.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db03c13dd8f044b9b897acc223a3e0f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/266367 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1978b3eb5b1b403496e795e0e374d235"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec9a3a30bf244d0481927e554358f99c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/13920 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3774235b80e4a6ab455742d5e74d507"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5133 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85f8ed8948024a85b0dc71267d7a9660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/18412 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5485d5be9434acf8320b8e19c6b5494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2664 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23e73930dc9240859947ace4b5efc567"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5127 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bb11527d7654f5c9912bebeffed44bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18412 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18203b6a65b47fd8a8803f31fdc1492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2664 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5ad19bf690f407183ff6814d724536c"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['content'],\n        num_rows: 23582\n    })\n    test: Dataset({\n        features: ['content'],\n        num_rows: 2621\n    })\n})\nEjemplo: {'content': \"{{Ficha de taxón\\n| name = Artabotrys\\n| image = Artabotrys hexapetalus Blanco1.194.png\\n| image_width = 250px\\n| image_caption = Artabotrys hexapetalus\\n| regnum = Plantae\\n| divisio = Magnoliophyta\\n| classis = Magnoliopsida\\n| ordo = Magnoliales\\n| familia = Annonaceae\\n| genus = Artabotrys\\n| genus_authority = R.Br., 1820. \\n| subdivision_ranks = Especies\\n| subdivision = \\nVer texto.\\n| synonyms =\\n Ropalopetalum\\n}}Artabotrys es un género de plantas de la familia de las Annonáceas, orden Magnoliales, subclase Magnólidas, subdivisión Magnoliophytina, división Spermatophyta.\\nDescripción\\nSon arbustos trepadores o erectos, raramente árboles pequeños. Indumento de pelos simples o ausentes. Flores bisexuales, cimas pediceladas, solitarias o en extra-axilares, raramente terminales, sobre todo con gruesos y ganchudos pedúnculos; brácteas y bractéolas pequeñas, generalmente de hoja caduca. Fruto generalmente carnoso, cilíndrico o elipsoidal.\\n Hábitat \\nEs natural de las regiones Paleotropicales.\\nTaxonomía\\nEl género fue descrito por Robert Brown y publicado en Botanical Register; consisting of coloured . . . 5: , pl. 423. 1820.  La especie tipo es: Artabotrys odoratissimus R. Br. ex Ker Gawl.\\n\\n Especies \\n\\n Artabotrys arachnoides J. Sinclair \\n\\n Artabotrys harmandii Finet et Gagnep. \\n\\n Artabotrys havilandii Ridl. \\n\\n Artabotrys le-testui Pellegr. \\n\\n Artabotrys luteus Elmer \\n\\n Artabotrys palustris Louis ex Boutique \\n\\n Artabotrys petelotii Merr. \\n\\n Artabotrys pilosus Merr. et Chun \\n\\n Artabotrys robustus Louis ex Boutique \\n\\n Artabotrys stenopetalus Engl. et Diels \\n\\n Artabotrys stenopetalus Merr. et Chun \\n\\n Artabotrys stolonifera Elmer \\n\\n Artabotrys suaveolens'' Blume\\n\\nReferencias\\n\\nEnlaces externos \\n\\nAnnonaceae\"}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"La combinación de corpus permitió integrar exitosamente los tres conjuntos de datos en español (poesía, Wikipedia y noticias MLSUM). Tras la descarga y filtrado inicial, se eliminaron los registros vacíos y se homogenizó la estructura bajo la columna “content”. El resultado final fue un DatasetDict con un total de 2.918 registros, de los cuales el 90% (2.626 ejemplos) se destinó al entrenamiento y el 10% (292 ejemplos) a validación. En la impresión de consola se observa además un ejemplo de texto del dataset combinado, correspondiente a una entrada enciclopédica, lo que confirma la correcta integración de las fuentes. Este resultado garantiza un corpus más diverso y robusto, capaz de enriquecer el vocabulario del modelo y mejorar su capacidad de generalización en comparación con el uso exclusivo del corpus poético.","metadata":{}},{"cell_type":"markdown","source":"# Tokenización del dataset combinado","metadata":{}},{"cell_type":"markdown","source":"Se lleva a cabo la tokenización del dataset combinado y la preparación de los datos para el entrenamiento del modelo. Para ello, se definió una función de preprocesamiento que transforma el contenido textual en secuencias de tokens con una longitud máxima de 256, aplicando truncamiento y padding para homogeneizar las entradas. Una vez aplicado este proceso al conjunto de entrenamiento, se eliminaron las columnas innecesarias conservando únicamente los identificadores de tokens (input_ids). Posteriormente, el dataset resultante se dividió nuevamente en subconjuntos de entrenamiento (90%) y validación (10%), y se configuró en formato compatible con PyTorch.\n","metadata":{}},{"cell_type":"code","source":"# ============================\n# 3) TOKENIZADOR Y MODELO\n# ============================\ndef preprocess_function(max_len):\n    def _preprocess_function(examples):\n        return tokenizer(examples['content'], max_length=max_len, truncation=True, padding='max_length')\n    return _preprocess_function\n\ndataset.reset_format()\ntokenized_dataset = dataset['train'].map(preprocess_function(max_len=256), batched=True)\ntokenized_dataset = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col != 'input_ids'])\ntokenized_dataset = tokenized_dataset.train_test_split(train_size=0.9)\ntokenized_dataset.set_format('torch')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T21:38:39.780970Z","iopub.execute_input":"2025-09-21T21:38:39.781716Z","iopub.status.idle":"2025-09-21T21:39:49.512457Z","shell.execute_reply.started":"2025-09-21T21:38:39.781695Z","shell.execute_reply":"2025-09-21T21:39:49.511892Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23582 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec16e04766c49109d2abb1e17a3e090"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"Lo anterior confirma la correcta ejecución del mapeo sobre 23.582 ejemplos, con un procesamiento aproximado de 342 ejemplos por segundo. De esta manera, el corpus enriquecido y tokenizado quedó listo para iniciar la fase de ajuste fino del modelo, asegurando tanto la calidad de los datos como su compatibilidad con el framework de entrenamiento.","metadata":{}},{"cell_type":"markdown","source":"# Entrenamiento del modelo con el dataset combinado y monitoreo de perplejidad","metadata":{}},{"cell_type":"markdown","source":"A continuación se ejecuta el entrenamiento del modelo con el corpus combinado, incorporando el callback de perplejidad para monitorear el desempeño. Se cargó nuevamente el modelo base gpt2-small-spanish junto con su tokenizador, y se configuraron parámetros de ajuste con 20 épocas de entrenamiento, un batch size de 32, tasa de aprendizaje de 3e-5, weight decay de 0.05 y acumulación de gradientes cada 2 pasos para optimizar recursos en GPU. Asimismo, se definió una estrategia de evaluación y guardado cada 200 pasos, conservando únicamente los dos mejores checkpoints.\n\nEl callback personalizado calculó e imprimió la perplejidad (PPL) a partir de la pérdida de validación, permitiendo evaluar en tiempo real la capacidad predictiva del modelo. Durante la ejecución, el entrenamiento procesó el dataset tokenizado dividido en entrenamiento y prueba, aplicando el esquema de causal language modeling sin enmascaramiento de tokens.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainerCallback\nimport math\n\n# ============================\n# CALLBACK PERSONALIZADO\n# ============================\nclass PerplexityCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        if metrics and \"eval_loss\" in metrics:\n            ppl = math.exp(metrics[\"eval_loss\"])\n            print(f\"\\n🔎 Step {state.global_step}: Eval Loss = {metrics['eval_loss']:.4f} | PPL = {ppl:.2f}\")\n\nmodel_name = \"datificate/gpt2-small-spanish\"\nmodel, tokenizer = init_model_and_tokenizer(model_name, device=device)\n\nbatch_size = 32\n\ntraining_args = TrainingArguments(\n    output_dir=\"./hf-gpt\",\n    run_name=\"poesia-gpt2\",\n    overwrite_output_dir=True,\n    num_train_epochs=20,#20\n    learning_rate=3e-5,#3e-5, 5e-5\n    per_device_eval_batch_size=batch_size,\n    per_device_train_batch_size=batch_size,\n    weight_decay=0.05,#0.01\n    warmup_steps=1000, #1500, 100\n    evaluation_strategy=\"steps\",  \n    eval_steps=200,               \n    save_steps=200,               \n    logging_strategy=\"steps\",\n    logging_steps=50,\n    logging_first_step=True,\n    save_strategy=\"steps\",    \n    save_total_limit=2,\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    disable_tqdm=False,\n    fp16=True,\n    gradient_accumulation_steps = 2 #Nuevo\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    callbacks=[PerplexityCallback]   # 👈 Agregamos el callback\n)\n\n# ============================\n# 4) ENTRENAMIENTO\n# ============================\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T21:39:49.513132Z","iopub.execute_input":"2025-09-21T21:39:49.513353Z","iopub.status.idle":"2025-09-22T01:58:21.076407Z","shell.execute_reply.started":"2025-09-21T21:39:49.513337Z","shell.execute_reply":"2025-09-22T01:58:21.075761Z"}},"outputs":[{"name":"stdout","text":"✅ Modelo y tokenizer listos\npad_token: <|endoftext|> 0\nbos_token: <|endoftext|> 0\neos_token: <|endoftext|> 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6640' max='6640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6640/6640 4:18:27, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>3.655100</td>\n      <td>3.444876</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.540900</td>\n      <td>3.358124</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.474800</td>\n      <td>3.306393</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.401900</td>\n      <td>3.272259</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.356400</td>\n      <td>3.244515</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.332100</td>\n      <td>3.221701</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.256900</td>\n      <td>3.205233</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>3.222500</td>\n      <td>3.192782</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>3.207100</td>\n      <td>3.180860</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>3.195700</td>\n      <td>3.173244</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>3.144300</td>\n      <td>3.168532</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>3.133100</td>\n      <td>3.160813</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>3.115900</td>\n      <td>3.155092</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>3.109800</td>\n      <td>3.153128</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>3.095200</td>\n      <td>3.148079</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>3.045700</td>\n      <td>3.144797</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>3.039400</td>\n      <td>3.140820</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>3.033500</td>\n      <td>3.139832</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>3.022200</td>\n      <td>3.138320</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>3.003200</td>\n      <td>3.137174</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>3.010900</td>\n      <td>3.134299</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>2.993500</td>\n      <td>3.132944</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>2.990400</td>\n      <td>3.130404</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>2.987500</td>\n      <td>3.130050</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.984700</td>\n      <td>3.128673</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>2.970600</td>\n      <td>3.129244</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>2.955100</td>\n      <td>3.127755</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>2.956200</td>\n      <td>3.126264</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>2.949500</td>\n      <td>3.125878</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>2.957100</td>\n      <td>3.126255</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>2.931500</td>\n      <td>3.125736</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>2.945300</td>\n      <td>3.125950</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>2.944600</td>\n      <td>3.125678</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n🔎 Step 200: Eval Loss = 3.4449 | PPL = 31.34\n\n🔎 Step 400: Eval Loss = 3.3581 | PPL = 28.74\n\n🔎 Step 600: Eval Loss = 3.3064 | PPL = 27.29\n\n🔎 Step 800: Eval Loss = 3.2723 | PPL = 26.37\n\n🔎 Step 1000: Eval Loss = 3.2445 | PPL = 25.65\n\n🔎 Step 1200: Eval Loss = 3.2217 | PPL = 25.07\n\n🔎 Step 1400: Eval Loss = 3.2052 | PPL = 24.66\n\n🔎 Step 1600: Eval Loss = 3.1928 | PPL = 24.36\n\n🔎 Step 1800: Eval Loss = 3.1809 | PPL = 24.07\n\n🔎 Step 2000: Eval Loss = 3.1732 | PPL = 23.88\n\n🔎 Step 2200: Eval Loss = 3.1685 | PPL = 23.77\n\n🔎 Step 2400: Eval Loss = 3.1608 | PPL = 23.59\n\n🔎 Step 2600: Eval Loss = 3.1551 | PPL = 23.46\n\n🔎 Step 2800: Eval Loss = 3.1531 | PPL = 23.41\n\n🔎 Step 3000: Eval Loss = 3.1481 | PPL = 23.29\n\n🔎 Step 3200: Eval Loss = 3.1448 | PPL = 23.21\n\n🔎 Step 3400: Eval Loss = 3.1408 | PPL = 23.12\n\n🔎 Step 3600: Eval Loss = 3.1398 | PPL = 23.10\n\n🔎 Step 3800: Eval Loss = 3.1383 | PPL = 23.07\n\n🔎 Step 4000: Eval Loss = 3.1372 | PPL = 23.04\n\n🔎 Step 4200: Eval Loss = 3.1343 | PPL = 22.97\n\n🔎 Step 4400: Eval Loss = 3.1329 | PPL = 22.94\n\n🔎 Step 4600: Eval Loss = 3.1304 | PPL = 22.88\n\n🔎 Step 4800: Eval Loss = 3.1300 | PPL = 22.88\n\n🔎 Step 5000: Eval Loss = 3.1287 | PPL = 22.84\n\n🔎 Step 5200: Eval Loss = 3.1292 | PPL = 22.86\n\n🔎 Step 5400: Eval Loss = 3.1278 | PPL = 22.82\n\n🔎 Step 5600: Eval Loss = 3.1263 | PPL = 22.79\n\n🔎 Step 5800: Eval Loss = 3.1259 | PPL = 22.78\n\n🔎 Step 6000: Eval Loss = 3.1263 | PPL = 22.79\n\n🔎 Step 6200: Eval Loss = 3.1257 | PPL = 22.78\n\n🔎 Step 6400: Eval Loss = 3.1260 | PPL = 22.78\n\n🔎 Step 6600: Eval Loss = 3.1257 | PPL = 22.78\n","output_type":"stream"},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6640, training_loss=3.12761747413371, metrics={'train_runtime': 15509.6357, 'train_samples_per_second': 27.368, 'train_steps_per_second': 0.428, 'total_flos': 5.545400795136e+16, 'train_loss': 3.12761747413371, 'epoch': 20.0})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def generar_texto(model, tokenizer, prompt=\"En el silencio de la noche\"):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(\n        **inputs,\n        max_length=120,\n        num_return_sequences=5,\n        temperature=1.1,\n        top_k=30,\n        top_p=0.95,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id   # 🔑 ya definido arriba\n    )\n    for i, out in enumerate(outputs):\n        print(f\"=== Poema {i+1} ===\")\n        print(tokenizer.decode(out, skip_special_tokens=True))\n        print()\n\ngenerar_texto(model, tokenizer, \"Había una vez un poeta que soñaba\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T01:58:21.077221Z","iopub.execute_input":"2025-09-22T01:58:21.077481Z","iopub.status.idle":"2025-09-22T01:58:22.644530Z","shell.execute_reply.started":"2025-09-22T01:58:21.077461Z","shell.execute_reply":"2025-09-22T01:58:22.643688Z"}},"outputs":[{"name":"stdout","text":"=== Poema 1 ===\nHabía una vez un poeta que soñaba\ncon contar la historia de un país.\nCuando se supo de él, me dio vergüenza.\nY no sé si en los días siguientes\nno era más, más triste,\nen el tiempo.\nCuando se dio la historia.\nY cuando se empezó a contar.\nCuando se terminó.\nY cuando era de hablar.\nEntonces fue una vez... \n¡ay!, se me dijera,\ny la historia era el recuerdo\nde nuestro pueblo, y que nos dijeron las cosas.\n\nSe nos dio vergüenza,\n\n\n=== Poema 2 ===\nHabía una vez un poeta que soñaba\nun viejo poeta que hablaba.\n\n¿En qué nos esperamos?\n¿Algún tiempo la conciencia nos envía\na las calles para hablar\ncon los sonidos.\n¿En qué nos esperamos?\n¿Cómo, para que la conciencia nos haya\na escuchar el mundo, no hay forma en nuestro mundo?\n\nY en qué nos esperamos se trata\nen un encuentro en donde no hay formas\nen nuestro mundo.\n\n\n¿Cómo nos esperamos?\n¿La consciencia nos ofrezca\nque somos iguales.\n\n=== Poema 3 ===\nHabía una vez un poeta que soñaba con su futuro y esperaba que un día pudiese escribir su nombre. No sabía cómo hacer el viejo sueño de los amantes, pero las noches estaban con él. En la ciudad, con un reloj de bolsillo de agua, un amigo de la infancia, le dijo a su familia que un amigo que vivía con otra mujer sería el más próximo. Así el hombre de hoy fue como él, una niña blanca en medio de la oscuridad de la noche,\nde piel pálida y pálida. Su poesía siempre ha ido unida al amor.   \n   \n\n=== Poema 4 ===\nHabía una vez un poeta que soñaba \nde alguna forma: de algún oficio. \nHay una vez que a su favor una vez que la \nyerezada \nno fue en vano \nni en vano: \n¡Qué bien ha sucedido! \nSegura que haya hecho algo; \nque ha hecho algo, que ha hecho algo, \ny que ha hecho algo. Pero lo ha hecho, \nque ha hecho algo... Y no\nno lo ha hecho ni lo ha hecho. \nPero hay algo en que se ha puesto\n\n=== Poema 5 ===\nHabía una vez un poeta que soñaba. Era Jesús de Nazaret, de la ciudad. El poeta que soñaba era José María Juárez. Y hoy no existe. El verso se ha ido perdiendo con los años. De la biblioteca se sabe que tenía ya 25 poemas. Y hoy está lloviendo agua. Sólo hay dos libros que aún se han ido imprimiendo. \"Nos fuimos los poetas a ver, por Dios nos vamos bien\", dice. Y así, no puede seguir siendo una realidad la realidad, porque se ha ido borrando sus textos. En esta ocasión ya lo ha pasado Pablo Fernández\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Tras el entrenamiento del modelo gpt2-small-spanish con el corpus combinado (poesía, Wikipedia y noticias), los resultados muestran una mejora sustancial frente al entrenamiento realizado únicamente con poesía. A lo largo de las 20 épocas configuradas, el modelo alcanzó un training loss final de 2.94 y un validation loss de 3.12, valores mucho más bajos que en la primera prueba. La métrica de perplejidad (PPL) final se situó en 22.78, lo que representa una reducción significativa en comparación con la PPL de 47.69 obtenida con el dataset exclusivo de poesía.\n\nTras el entrenamiento del modelo gpt2-small-spanish con el corpus combinado (poesía, Wikipedia y noticias), se evidenció una mejora relevante frente al entrenamiento inicial realizado únicamente con poesía. Durante las 20 épocas configuradas, el modelo alcanzó un training loss final de 2.94 y un validation loss de 3.12, métricas que reflejan un aprendizaje más estable y eficiente. La perplejidad (PPL) final se situó en 22.78, lo que representa una reducción significativa en comparación con el valor de 47.69 obtenido cuando se entrenó únicamente con el corpus de poesía, confirmando el aporte positivo de integrar fuentes textuales más diversas.\n\nPosteriormente se puso a prueba la capacidad generativa del modelo mediante la función generar_texto, configurada con parámetros de muestreo diseñados para promover diversidad y fluidez en los resultados (max_length=120, num_return_sequences=5, temperature=1.1, top_k=30, top_p=0.95). Con el prompt inicial “Había una vez un poeta que soñaba”, el modelo produjo cinco composiciones poéticas distintas, donde los textos generados mostraron avances claros respecto al modelo entrenado únicamente con poesía, manteniendo una estructura más coherente y abordando temáticas más diversas, entre ellas el paso del tiempo, la esperanza, la familia, la vida urbana e incluso referencias históricas o religiosas. No obstante, todavía se observaron repeticiones y fragmentos con menor coherencia semántica, lo que evidencia que, aunque el corpus combinado mejoró de manera notable la calidad de la generación, persisten limitaciones propias del tamaño del dataset y de la arquitectura del modelo.","metadata":{}},{"cell_type":"markdown","source":"# Resultados","metadata":{}},{"cell_type":"markdown","source":"* La inclusión de Wikipedia y noticias junto con el corpus poético permitió reducir la perplejidad de 47.69 a 20.94, lo que confirma que un dataset más diverso contribuye a mejorar la capacidad de generalización del modelo.\n\n* Los valores finales de training loss (2.94) y validation loss (3.12) reflejan un aprendizaje más controlado y menos propenso al sobreajuste, en comparación con el entrenamiento inicial basado únicamente en poesía.\n\n* Los poemas generados con el corpus combinado presentan estructuras más coherentes y temáticas variadas, lo que indica un mayor dominio del lenguaje y una mejor adaptación a distintos contextos narrativos.\n\n* A pesar de las mejoras, aún se observan repeticiones y fragmentos con baja coherencia semántica, lo que sugiere que el modelo requiere más datos o ajustes adicionales para alcanzar un nivel óptimo en la generación de texto poético, lo que a su vez tambien evidencia la capacidad de computo que se requiere para mejorarlo.\n\n* El experimento confirma que la estrategia de combinar corpus heterogéneos enriquece el vocabulario y la expresividad del modelo, siendo un paso necesario para futuros entrenamientos que busquen resultados más consistentes y creativos en generación de lenguaje natural.","metadata":{}}]}