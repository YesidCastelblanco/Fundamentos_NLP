{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n\n<div style=\"text-align: center; font-family: Arial, sans-serif; margin-top: 50px;\">\n    <h1 style=\"font-size: 36px; font-weight: bold;\"><b>Prácticas de NLP</b></h1>\n    <h2 style=\"font-size: 28px; color: #2E86C1;\"><b>NLP con Long-Short Term Memory (LSTM)</b></h2>\n    <p style=\"font-size: 20px; margin-top: 30px;\">\n        <b>Materia:</b> Procesamiento de Lenguaje Natural<br>\n        <b>Estudiantes:</b> Albin Rivera y Yesid Castelblanco<br>\n        <b>Fecha:</b> 23 de Agosto de 2025\n    </p>\n</div>\n\n---","metadata":{"id":"A8HQapXYKNfD"}},{"cell_type":"markdown","source":"## Referencias\n- Dataset: [Fake News Corpus Spanish](https://huggingface.co/mariagrandury/fake_news_corpus_spanish)  \n- Librerías: Hugging Face `datasets`, Pandas, warnings  ","metadata":{"id":"iU6aBoHEk-Sj"}},{"cell_type":"markdown","source":"# **1. Configuración Inicial**\n---","metadata":{"id":"ynAI4Y3GKqHf"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nEn esta primera sección se realiza la configuración del entorno de trabajo. Se importan las librerías necesarias para el procesamiento de lenguaje natural (NLTK, Transformers, Datasets), el manejo de datos (NumPy, Pandas), y el entrenamiento de modelos de Deep Learning con PyTorch y PyTorch Lightning. Además, se preparan métricas de evaluación como exactitud, precisión, recall y F1-score, que serán utilizadas en el proceso de entrenamiento y validación del modelo.","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nimport sys\nimport torch\nimport numpy as np\nimport pandas as pd\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import AutoTokenizer, logging as hf_logging\nfrom datasets import load_dataset\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torchmetrics.classification import Accuracy, Precision, Recall, F1Score\nfrom torch import nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:37:19.245037Z","iopub.execute_input":"2025-08-24T00:37:19.245301Z","iopub.status.idle":"2025-08-24T00:37:34.666081Z","shell.execute_reply.started":"2025-08-24T00:37:19.245282Z","shell.execute_reply":"2025-08-24T00:37:34.665476Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nAdicional, se configuran parámetros específicos para asegurar compatibilidad entre entornos como Colab o Kaggle, se ajusta el número de workers, se descargan las stopwords en español y se deshabilitan ciertas optimizaciones para evitar conflictos con librerías de GPU. También se instalan las dependencias necesarias y se suprimen advertencias innecesarias que podrían dificultar la lectura de resultados.","metadata":{}},{"cell_type":"code","source":"# Configuraciones para compatibilidad con Kaggle y Colab\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept ImportError:\n    IN_COLAB = False\n\nif IN_COLAB:\n    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n    num_workers = 0\nelse:\n    num_workers = 4\n\n# Descargar stopwords para español\nnltk.download('stopwords')\nstop_words = set(stopwords.words('spanish'))\n\n# Deshabilitar oneDNN para evitar errores en LSTM\ntorch.backends.mkldnn.enabled = False\nos.environ[\"ONEDNN_VERBOSE\"] = \"all\"\n\n# Deshabilitar XLA para evitar conflictos con cuFFT/cuDNN/cuBLAS\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/usr/lib/cuda\"\n\n# Suprimir advertencias y configurar tokenización paralela\nwarnings.filterwarnings('ignore')\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Instalar dependencias\nprint(\"📦 Instalando dependencias...\")\n!pip install torch==2.3.0 transformers==4.38.2 datasets==2.14.5 pytorch-lightning==2.2.1 torchmetrics==1.0.3 nlpaug==1.1.11 --quiet\n\n# Configurar logging de Hugging Face\nhf_logging.set_verbosity_error()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:37:34.667154Z","iopub.execute_input":"2025-08-24T00:37:34.667560Z","iopub.status.idle":"2025-08-24T00:40:30.043709Z","shell.execute_reply.started":"2025-08-24T00:37:34.667542Z","shell.execute_reply":"2025-08-24T00:40:30.042720Z"}},"outputs":[{"name":"stdout","text":"📦 Instalando dependencias...\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m968.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m612.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.3.0 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.3.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2023.6.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **2. Preprocesamiento de textos**","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nAqui se define una función para limpiar y normalizar el texto, convirtiéndolo a minúsculas, eliminando stopwords y conservando solo aquellos signos de puntuación que son relevantes en el idioma español (como exclamaciones e interrogaciones), permitiendo reducir el ruido en los datos y mejorar el rendimiento de los modelos posteriores.","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    \"\"\"Limpia el texto: minúsculas, conserva puntuación relevante, elimina stopwords.\"\"\"\n    text = text.lower()\n    # Conservar signos de exclamación e interrogación\n    text = ' '.join(word for word in text.split() if word not in stop_words)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.044940Z","iopub.execute_input":"2025-08-24T00:40:30.045284Z","iopub.status.idle":"2025-08-24T00:40:30.050223Z","shell.execute_reply.started":"2025-08-24T00:40:30.045245Z","shell.execute_reply":"2025-08-24T00:40:30.049500Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# **3. Cargar y explorar el Dataset**","metadata":{"id":"hTg_ajBlMqzH"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nEn esta parte se carga el dataset fake_news_corpus_spanish desde HuggingFace, se convierte en un DataFrame de Pandas y se aplican transformaciones iniciales como la conversión de la variable CATEGORY a valores binarios y el preprocesamiento de los textos. Además, se calculan estadísticas básicas de longitud de los textos y la distribución de las categorías, lo cual ayuda a detectar posibles desbalances en las clases.","metadata":{}},{"cell_type":"code","source":"def load_and_explore_dataset():\n    \"\"\"Carga el dataset y muestra estadísticas básicas.\"\"\"\n    print(\"📥 Cargando 'fake_news_corpus_spanish' dataset (split: test)...\")\n    try:\n        dataset = load_dataset(\"mariagrandury/fake_news_corpus_spanish\", split=\"test\")\n        df = dataset.to_pandas()\n    except Exception as e:\n        print(f\"Error al cargar el dataset: {e}\")\n        sys.exit(1)\n\n    # Convertir CATEGORY a binario y preprocesar textos\n    df['CATEGORY'] = df['CATEGORY'].astype(int)\n    df['TEXT'] = df['TEXT'].apply(preprocess_text)\n\n    # Mostrar información del dataset\n    print(\"\\nColumnas del Dataset:\", df.columns.tolist())\n    print(\"\\nPrimeras Filas:\\n\", df.head())\n    print(\"\\nDistribución de Categorías:\\n\", df['CATEGORY'].value_counts())\n    print(f\"\\nTotal de Textos: {len(df)}\")\n\n    # Estadísticas de longitud de texto\n    text_lengths = [len(text) for text in df['TEXT']]\n    print(f\"Texto más corto: {min(text_lengths)}\")\n    print(f\"Texto más largo: {max(text_lengths)}\")\n    print(f\"Longitud promedio: {np.mean(text_lengths):.2f}\")\n\n    return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.052039Z","iopub.execute_input":"2025-08-24T00:40:30.052281Z","iopub.status.idle":"2025-08-24T00:40:30.072718Z","shell.execute_reply.started":"2025-08-24T00:40:30.052254Z","shell.execute_reply":"2025-08-24T00:40:30.072003Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# **4. Dividir el Dataset**","metadata":{"id":"ZyBrxkSlNSfT"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nUna vez cargado y explorado, el dataset se divide en subconjuntos para permitir un entrenamiento robusto y evaluaciones confiables. En esta sección se define una función que separa el dataset en tres particiones las cuales son entrenamiento (80%), validación (10%) y prueba (10%).","metadata":{}},{"cell_type":"code","source":"def split_dataset(dataset):\n    \"\"\"Divide el dataset en conjuntos de entrenamiento, validación y prueba.\"\"\"\n    dataset_size = len(dataset)\n    train_size = int(0.8 * dataset_size)\n    val_size = int(0.1 * dataset_size)\n    test_size = dataset_size - train_size - val_size\n\n    train_subset, val_subset, test_subset = random_split(\n        dataset,\n        lengths=[train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n\n    print(f\"✅ Train: {len(train_subset)}, Val: {len(val_subset)}, Test: {len(test_subset)}\")\n    return train_subset, val_subset, test_subset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.073389Z","iopub.execute_input":"2025-08-24T00:40:30.073558Z","iopub.status.idle":"2025-08-24T00:40:30.090718Z","shell.execute_reply.started":"2025-08-24T00:40:30.073544Z","shell.execute_reply":"2025-08-24T00:40:30.090014Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# **5. Clase de Dataset personalizado**","metadata":{"id":"sXoHrRHyNj1H"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nEn este punto se implementa una clase personalizada de Dataset para manejar el corpus de noticias falsas en español. La clase se encarga de tokenizar los textos, aplicar truncamiento y padding hasta una longitud máxima definida, y asociar cada texto con su etiqueta correspondiente.","metadata":{}},{"cell_type":"code","source":"class FakeNewsCorpusSpanishDataset(Dataset):\n    \"\"\"Dataset personalizado para clasificación de noticias falsas.\"\"\"\n    def __init__(self, tokenizer, dataset, seq_length=512):\n        self.tokenizer = tokenizer\n        self.dataset = dataset\n        self.seq_length = seq_length\n        self.id_2_class_map = {0: 'False', 1: 'True'}\n        self.class_2_id_map = {'False': 0, 'True': 1}\n        self.num_classes = len(self.id_2_class_map)\n\n    def __getitem__(self, index):\n        \"\"\"Tokeniza el texto preprocesado y retorna tensores.\"\"\"\n        text = preprocess_text(self.dataset[index]['TEXT'])\n        label = self.dataset[index]['CATEGORY']\n        label = self.class_2_id_map['True' if label == 1 else 'False']\n\n        tokenized = self.tokenizer(\n            text,\n            max_length=self.seq_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': tokenized['input_ids'].squeeze(0),\n            'attention_mask': tokenized['attention_mask'].squeeze(0),\n            'y': torch.tensor(label, dtype=torch.long)\n        }\n\n    def __len__(self):\n        return len(self.dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.091598Z","iopub.execute_input":"2025-08-24T00:40:30.092241Z","iopub.status.idle":"2025-08-24T00:40:30.105683Z","shell.execute_reply.started":"2025-08-24T00:40:30.092223Z","shell.execute_reply":"2025-08-24T00:40:30.104939Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **6. Definición del modelo LSTM**","metadata":{"id":"XrDep4OXNsDL"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nAquí se define el modelo de clasificación basado en una red recurrente LSTM (Long Short-Term Memory). Este modelo utiliza embeddings para representar las palabras, capas LSTM bidireccionales para capturar dependencias en ambas direcciones del texto, normalización por lotes para estabilizar el entrenamiento y capas totalmente conectadas con dropout para mejorar la generalización.","metadata":{}},{"cell_type":"code","source":"class LSTMClassifier(nn.Module):\n    \"\"\"Clasificador basado en LSTM para detección de noticias falsas.\"\"\"\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n\n    def forward(self, input_ids):\n        embedded = self.embedding(input_ids)\n        lstm_out, _ = self.lstm(embedded)\n        pooled = lstm_out[:, -1, :]\n        pooled = self.batch_norm(pooled)\n        pooled = self.dropout(pooled)\n        logits = self.fc(pooled)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.106493Z","iopub.execute_input":"2025-08-24T00:40:30.106731Z","iopub.status.idle":"2025-08-24T00:40:30.123847Z","shell.execute_reply.started":"2025-08-24T00:40:30.106708Z","shell.execute_reply":"2025-08-24T00:40:30.123113Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# **7. Módulo de PyTorch Lightning**","metadata":{"id":"AElD5mgpN30y"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nPara simplificar el entrenamiento, validación y prueba del modelo, se crea un módulo basado en PyTorch Lightning. Esta clase integra el modelo LSTM con la función de pérdida, optimizador, programador de tasa de aprendizaje y métricas de evaluación. Además, incluye métodos específicos para cada fase del entrenamiento y predicción, asegurando un flujo estandarizado y reproducible.","metadata":{}},{"cell_type":"code","source":"class SpanishNewsClassifierWithLSTM(pl.LightningModule):\n    \"\"\"Módulo de PyTorch Lightning para entrenar el clasificador LSTM.\"\"\"\n    def __init__(self, vocab_size, num_classes, embed_dim=256, hidden_dim=128, lr=3e-4):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes)\n        self.loss_fn = nn.CrossEntropyLoss()\n\n        # Inicializar pesos\n        for module in self.model.modules():\n            if isinstance(module, nn.LSTM):\n                nn.init.xavier_uniform_(module.weight_ih_l0)\n                nn.init.xavier_uniform_(module.weight_hh_l0)\n                nn.init.xavier_uniform_(module.weight_ih_l1)\n                nn.init.xavier_uniform_(module.weight_hh_l1)\n            elif isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n\n        # Métricas para clasificación binaria\n        self.train_acc = Accuracy(task='binary', num_classes=num_classes)\n        self.val_acc = Accuracy(task='binary', num_classes=num_classes)\n        self.test_acc = Accuracy(task='binary', num_classes=num_classes)\n        self.test_precision = Precision(task='binary', num_classes=num_classes)\n        self.test_recall = Recall(task='binary', num_classes=num_classes)\n        self.test_f1 = F1Score(task='binary', num_classes=num_classes)\n\n    def forward(self, input_ids):\n        return self.model(input_ids)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch['input_ids'], batch['y']\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        probs = logits.softmax(dim=-1)[:, 1]\n        self.train_acc(probs, y)\n        self.log('train_loss', loss, prog_bar=True)\n        self.log('train_acc', self.train_acc, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch['input_ids'], batch['y']\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        probs = logits.softmax(dim=-1)[:, 1]\n        self.val_acc(probs, y)\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', self.val_acc, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch['input_ids'], batch['y']\n        logits = self(x)\n        probs = logits.softmax(dim=-1)[:, 1]\n        self.test_acc(probs, y)\n        self.test_precision(probs, y)\n        self.test_recall(probs, y)\n        self.test_f1(probs, y)\n        self.log('test_acc', self.test_acc, prog_bar=True)\n        self.log('test_precision', self.test_precision, prog_bar=True)\n        self.log('test_recall', self.test_recall, prog_bar=True)\n        self.log('test_f1', self.test_f1, prog_bar=True)\n\n    def predict_step(self, batch, batch_idx):\n        x = batch['input_ids']\n        logits = self(x)\n        probs = logits.softmax(dim=-1)\n        return probs\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}\n        }\n\n    def on_train_batch_end(self, outputs, batch, batch_idx):\n        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.124642Z","iopub.execute_input":"2025-08-24T00:40:30.124882Z","iopub.status.idle":"2025-08-24T00:40:30.139361Z","shell.execute_reply.started":"2025-08-24T00:40:30.124858Z","shell.execute_reply":"2025-08-24T00:40:30.138563Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# **8. Función de Predicción**","metadata":{"id":"xxa2Yt2LOCwv"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nCon el modelo ya entrenado, esta función permite evaluar textos nuevos para determinar si son verdaderos o falsos. Se aplica el mismo preprocesamiento y tokenización que en el entrenamiento, y luego se obtiene la predicción junto con la probabilidad asociada.","metadata":{}},{"cell_type":"code","source":"def predict_text(model, tokenizer, text, seq_length=512, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    \"\"\"Predice la clase de un texto nuevo.\"\"\"\n    model.eval()\n    model.to(device)\n    text = preprocess_text(text)\n    tokenized = tokenizer(\n        text,\n        max_length=seq_length,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n\n    input_ids = tokenized['input_ids'].to(device)\n    with torch.no_grad():\n        logits = model(input_ids)\n        probs = logits.softmax(dim=-1)\n        pred_class = torch.argmax(probs, dim=-1).item()\n\n    id_2_class = {0: 'False', 1: 'True'}\n    return id_2_class[pred_class], probs[0][pred_class].item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.140214Z","iopub.execute_input":"2025-08-24T00:40:30.140453Z","iopub.status.idle":"2025-08-24T00:40:30.180866Z","shell.execute_reply.started":"2025-08-24T00:40:30.140438Z","shell.execute_reply":"2025-08-24T00:40:30.180402Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# **9. Ejecución Principal y Metricas de Evaluación del Modelo**","metadata":{"id":"Lh42TI7xOR7o"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nFinalmente, se organiza todo el flujo en una función principal. Aquí se configuran los dispositivos de cómputo (CPU, GPU o TPU), se carga y divide el dataset, se inicializa el tokenizador y el modelo, y se definen los callbacks de entrenamiento como el EarlyStopping y la selección del mejor checkpoint. Tras el entrenamiento, el modelo se evalúa en el conjunto de prueba, se generan predicciones y se muestra un ejemplo de predicción aplicada a un texto real.","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"Función principal para ejecutar el pipeline.\"\"\"\n    # Verificar disponibilidad de GPU/TPU\n    if IN_COLAB and torch.cuda.is_available():\n        device = torch.device('cuda')\n        print(\"CUDA Disponible: True\")\n        print(\"Nombre de GPU:\", torch.cuda.get_device_name(0))\n    elif IN_COLAB and 'XLA' in torch.__version__:\n        device = torch.device('xla')\n        print(\"TPU Disponible: True\")\n    else:\n        device = torch.device('cpu')\n        print(\"CUDA/TPU no disponible, usando CPU\")\n\n    # Cargar dataset\n    dataset = load_and_explore_dataset()\n\n    # Inicializar tokenizador\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n    except Exception as e:\n        print(f\"Error al cargar el tokenizador: {e}\")\n        sys.exit(1)\n\n    # Dividir dataset y almacenar índices\n    train_subset, val_subset, test_subset = split_dataset(dataset)\n    test_indices = test_subset.indices\n\n    # Crear datasets personalizados\n    train_dataset = FakeNewsCorpusSpanishDataset(tokenizer, train_subset, seq_length=512)\n    val_dataset = FakeNewsCorpusSpanishDataset(tokenizer, val_subset, seq_length=512)\n    test_dataset = FakeNewsCorpusSpanishDataset(tokenizer, test_subset, seq_length=512)\n\n    # Crear dataloaders\n    batch_size = 16  # Reducir para gradientes más precisos\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=True)\n\n    # Inicializar modelo\n    model = SpanishNewsClassifierWithLSTM(\n        vocab_size=tokenizer.vocab_size,\n        num_classes=train_dataset.num_classes,\n        embed_dim=256,\n        hidden_dim=128,\n        lr=3e-4\n    )\n\n    # Configurar logger y callbacks\n    tb_logger = TensorBoardLogger('tb_logs', name='LSTMClassifier')\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=10, mode='min'),\n        ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, filename='best-checkpoint', dirpath='checkpoints')\n    ]\n\n    # Inicializar entrenador\n    trainer = pl.Trainer(\n        max_epochs=20,\n        accelerator=\"gpu\" if torch.cuda.is_available() else \"tpu\" if IN_COLAB and 'XLA' in torch.__version__ else \"cpu\",\n        devices=1,\n        logger=tb_logger,\n        callbacks=callbacks,\n        precision=\"16-mixed\" if torch.cuda.is_available() else \"16-true\" if IN_COLAB and 'XLA' in torch.__version__ else \"32-true\",\n        num_sanity_val_steps=0\n    )\n\n    # Entrenar modelo\n    print(\"🚀 Iniciando entrenamiento...\")\n    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\n    # Evaluar en conjunto de prueba\n    print(\"🔎 Evaluando en conjunto de prueba...\")\n    trainer.test(model, dataloaders=test_loader)\n\n    # Generar DataFrame con predicciones\n    print(\"📊 Generando DataFrame con predicciones...\")\n    model.eval()\n    predictions = trainer.predict(model, test_loader)\n    predictions = torch.cat(predictions, dim=0)\n    predictions = torch.argmax(predictions, dim=-1)\n    predictions = [train_dataset.id_2_class_map[pred.item()] for pred in predictions]\n\n    df = pd.DataFrame(data={\n        \"texto\": [dataset[i]['TEXT'] for i in test_indices],\n        \"categoría\": [train_dataset.id_2_class_map[dataset[i]['CATEGORY']] for i in test_indices],\n        \"predicción\": predictions[:len(test_indices)]\n    }, index=test_indices)\n    print(df.head(15))\n\n    # Ejemplo de predicción\n    sample_text = \"El presidente anunció nuevas medidas económicas.\"\n    prediction, confidence = predict_text(model, tokenizer, sample_text)\n    print(f\"\\n📝 Predicción de Ejemplo: '{sample_text}' -> {prediction} (Confianza: {confidence:.4f})\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:46:02.036503Z","iopub.execute_input":"2025-08-24T00:46:02.036786Z","iopub.status.idle":"2025-08-24T00:46:30.220835Z","shell.execute_reply.started":"2025-08-24T00:46:02.036766Z","shell.execute_reply":"2025-08-24T00:46:30.220106Z"}},"outputs":[{"name":"stdout","text":"CUDA Disponible: True\nNombre de GPU: Tesla P100-PCIE-16GB\n📥 Cargando 'fake_news_corpus_spanish' dataset (split: test)...\n\nColumnas del Dataset: ['ID', 'CATEGORY', 'TOPICS', 'SOURCE', 'HEADLINE', 'TEXT', 'LINK']\n\nPrimeras Filas:\n    ID  CATEGORY    TOPICS         SOURCE  \\\n0   1         1  Covid-19  El Economista   \n1   2         0  Política     El matinal   \n2   3         1  Política        El País   \n3   4         0  Política     AFPFactual   \n4   5         1  Sociedad   La Republica   \n\n                                            HEADLINE  \\\n0                       Covid-19: mentiras que matan   \n1  El Gobierno podrá acceder a las IPs de los móv...   \n2  La comunidad musulmana catalana denuncia a Vox...   \n3                                               None   \n4  El censo poblacional 2018 tendrá un costo de $...   \n\n                                                TEXT  \\\n0  control covid-19 sólo tema médicos resto perso...   \n1  gobierno pedro sánchez pablo iglesias encontra...   \n2  tres federaciones agrupan 90% mezquitas llevan...   \n3  dado conocer datos electorales preliminares pe...   \n4  primera fase censo virtual solo abril próximo ...   \n\n                                                LINK  \n0  https://www.eleconomista.com.mx/opinion/Covid-...  \n1  https://www.elmatinal.com/espana-ultima-hora/e...  \n2  https://elpais.com/espana/elecciones-catalanas...  \n3                         https://perma.cc/GYE6-SPMB  \n4  https://www.larepublica.co/economia/el-censo-p...  \n\nDistribución de Categorías:\n CATEGORY\n1    286\n0    286\nName: count, dtype: int64\n\nTotal de Textos: 572\nTexto más corto: 191\nTexto más largo: 18973\nLongitud promedio: 2301.06\n✅ Train: 457, Val: 57, Test: 58\n🚀 Iniciando entrenamiento...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0ab5c5aaa854b37987d3587d056f966"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"🔎 Evaluando en conjunto de prueba...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01d4466f76f744a09821914751d41461"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7291666865348816    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6285714507102966    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7333333492279053    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.550000011920929    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7291666865348816     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6285714507102966     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7333333492279053     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.550000011920929     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"📊 Generando DataFrame con predicciones...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bad2267d452440c1b0777f3cb36ba2c1"}},"metadata":{}},{"name":"stdout","text":"                                                 texto categoría predicción\n296  Las vacunas contra la COVID-19 -13 de ellas en...      True      False\n336  Si eres de los que ama tomarse una copita de v...      True      False\n99   Así lo reveló el estudio en el que participaro...      True       True\n516  Vacaciones de Semana Santa, buen tiempo, el ha...      True       True\n227  Varios vuelos con salida de Casablanca y desti...      True      False\n344  Se está haciendo viral tras recuperarse una gr...     False      False\n74   MASCARILLAS.... PREFERENTEMENTE PARA CARNAVAL ...     False      False\n207  El Laboratorio Biológico Chino de Wuhan es en ...     False      False\n480  El Papa Francisco podría estar enfermo de Coro...     False      False\n325  El expresidente del Gobierno Felipe González h...     False       True\n174  Una veintena de jóvenes del entorno abertzale ...     False      False\n431  El embarazo es parte de nuestra sexualidad y v...      True       True\n198  FUE TAN FÁCIL CONVENCER A CASI UN MUNDO ENTERO...     False      False\n533  A unas horas que Félix Salgado Macedonio sea r...      True      False\n48   Desde el mes pasado, la región de La Laguna de...     False      False\n\n📝 Predicción de Ejemplo: 'El presidente anunció nuevas medidas económicas.' -> False (Confianza: 0.6482)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# **10. Resultados**","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\n1. Los resultados obtenidos en la evaluación del modelo muestran un desempeño general aceptable, alcanzando una exactitud del 72.91%. Este valor indica que, en promedio, el modelo clasifica correctamente cerca de tres cuartas partes de los textos evaluados. La precisión alcanzó un valor de 0.73, lo que evidencia que, cuando el sistema identifica una noticia como verdadera o falsa, en la mayoría de los casos acierta en dicha clasificación. \n\n<p style=\"font-size: 16px;\">\n2. Sin embargo, el recall obtenido fue de 0.55, lo que revela que el modelo no logra identificar de manera efectiva todas las noticias falsas presentes en el conjunto de prueba, dejando escapar una proporción significativa de estas.\n\n<p style=\"font-size: 16px;\">\n3. El valor del F1-score, situado en 0.62, refleja un equilibrio moderado entre la precisión y el recall, pero también confirma la necesidad de fortalecer la sensibilidad del modelo para detectar noticias falsas sin sacrificar en exceso la precisión alcanzada. Al observar los ejemplos en el DataFrame generado, se aprecia que, aunque en muchos casos el modelo logra predecir de forma correcta, existen situaciones en las que clasifica erróneamente noticias con lenguaje ambiguo, sensacionalista o con contenido genérico, lo que impacta en el recall.Finalmente, al evaluar un texto de ejemplo —“El presidente anunció nuevas medidas económicas”—, el sistema lo clasificó como noticia falsa con una confianza de 64.82%. \n\n<p style=\"font-size: 16px;\">\n4. Este resultado ilustra tanto la capacidad del modelo para emitir predicciones sobre nuevos textos como la dificultad que enfrenta para interpretar contextos con baja carga informativa, donde aumenta el riesgo de errores de clasificación.\n","metadata":{}},{"cell_type":"markdown","source":"# **11. Conclusiones**","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\n1. Los resultados alcanzados muestran que el modelo constituye una base sólida para la detección automática de noticias falsas en español, dado que logra un nivel de exactitud por encima del 70%, lo cual es competitivo para un prototipo inicial desarrollado con un corpus limitado.\n\n<p style=\"font-size: 16px;\">\n2. Se observa que el modelo prioriza la precisión sobre el recall, lo que implica que es más confiable al momento de afirmar que una noticia es falsa, pero a costa de no detectar todas las que realmente lo son. Esto lo convierte en un clasificador conservador, útil en contextos donde los falsos positivos deben minimizarse.\n\n<p style=\"font-size: 16px;\">\n3. La brecha entre precisión y recall evidencia la necesidad de mejorar la cobertura del sistema, particularmente para capturar con mayor eficacia noticias falsas que poseen estructuras lingüísticas más diversas o complejas. Esto sugiere que una mayor variedad en los datos de entrenamiento podría beneficiar significativamente su rendimiento.\n\n<p style=\"font-size: 16px;\">\n4. El F1-score alcanzado indica que existe un balance moderado entre precisión y recall, pero que aún no es suficiente para un uso en escenarios críticos. Se puede optar por la optimización de hiperparámetros y la experimentación con arquitecturas más avanzadas, como transformers en español.\n\n<p style=\"font-size: 16px;\">\n5. El análisis de ejemplos concretos en el DataFrame revela que el modelo tiende a cometer errores en textos cortos, ambiguos o con expresiones genéricas. Esto pone en evidencia la importancia de considerar estrategias de data augmentation, para ampliar la diversidad lingüística en el entrenamiento.\n\n<p style=\"font-size: 16px;\">\n6. Si bien el prototipo actual presenta limitaciones, sus resultados confirman el potencial de aplicar técnicas de aprendizaje profundo en la detección de noticias falsas. Con ajustes en los datos, mejoras en la arquitectura y un refinamiento del pipeline.","metadata":{}}]}