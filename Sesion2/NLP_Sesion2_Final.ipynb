{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n\n<div style=\"text-align: center; font-family: Arial, sans-serif; margin-top: 50px;\">\n    <h1 style=\"font-size: 36px; font-weight: bold;\"><b>PrÃ¡cticas de NLP</b></h1>\n    <h2 style=\"font-size: 28px; color: #2E86C1;\"><b>NLP con Long-Short Term Memory (LSTM)</b></h2>\n    <p style=\"font-size: 20px; margin-top: 30px;\">\n        <b>Materia:</b> Procesamiento de Lenguaje Natural<br>\n        <b>Estudiantes:</b> Albin Rivera y Yesid Castelblanco<br>\n        <b>Fecha:</b> 23 de Agosto de 2025\n    </p>\n</div>\n\n---","metadata":{"id":"A8HQapXYKNfD"}},{"cell_type":"markdown","source":"## Referencias\n- Dataset: [Fake News Corpus Spanish](https://huggingface.co/mariagrandury/fake_news_corpus_spanish)  \n- LibrerÃ­as: Hugging Face `datasets`, Pandas, warnings  ","metadata":{"id":"iU6aBoHEk-Sj"}},{"cell_type":"markdown","source":"# **1. ConfiguraciÃ³n Inicial**\n---","metadata":{"id":"ynAI4Y3GKqHf"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nEn esta primera secciÃ³n se realiza la configuraciÃ³n del entorno de trabajo. Se importan las librerÃ­as necesarias para el procesamiento de lenguaje natural (NLTK, Transformers, Datasets), el manejo de datos (NumPy, Pandas), y el entrenamiento de modelos de Deep Learning con PyTorch y PyTorch Lightning. AdemÃ¡s, se preparan mÃ©tricas de evaluaciÃ³n como exactitud, precisiÃ³n, recall y F1-score, que serÃ¡n utilizadas en el proceso de entrenamiento y validaciÃ³n del modelo.","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nimport sys\nimport torch\nimport numpy as np\nimport pandas as pd\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import AutoTokenizer, logging as hf_logging\nfrom datasets import load_dataset\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torchmetrics.classification import Accuracy, Precision, Recall, F1Score\nfrom torch import nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:37:19.245037Z","iopub.execute_input":"2025-08-24T00:37:19.245301Z","iopub.status.idle":"2025-08-24T00:37:34.666081Z","shell.execute_reply.started":"2025-08-24T00:37:19.245282Z","shell.execute_reply":"2025-08-24T00:37:34.665476Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nAdicional, se configuran parÃ¡metros especÃ­ficos para asegurar compatibilidad entre entornos como Colab o Kaggle, se ajusta el nÃºmero de workers, se descargan las stopwords en espaÃ±ol y se deshabilitan ciertas optimizaciones para evitar conflictos con librerÃ­as de GPU. TambiÃ©n se instalan las dependencias necesarias y se suprimen advertencias innecesarias que podrÃ­an dificultar la lectura de resultados.","metadata":{}},{"cell_type":"code","source":"# Configuraciones para compatibilidad con Kaggle y Colab\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept ImportError:\n    IN_COLAB = False\n\nif IN_COLAB:\n    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n    num_workers = 0\nelse:\n    num_workers = 4\n\n# Descargar stopwords para espaÃ±ol\nnltk.download('stopwords')\nstop_words = set(stopwords.words('spanish'))\n\n# Deshabilitar oneDNN para evitar errores en LSTM\ntorch.backends.mkldnn.enabled = False\nos.environ[\"ONEDNN_VERBOSE\"] = \"all\"\n\n# Deshabilitar XLA para evitar conflictos con cuFFT/cuDNN/cuBLAS\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/usr/lib/cuda\"\n\n# Suprimir advertencias y configurar tokenizaciÃ³n paralela\nwarnings.filterwarnings('ignore')\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Instalar dependencias\nprint(\"ğŸ“¦ Instalando dependencias...\")\n!pip install torch==2.3.0 transformers==4.38.2 datasets==2.14.5 pytorch-lightning==2.2.1 torchmetrics==1.0.3 nlpaug==1.1.11 --quiet\n\n# Configurar logging de Hugging Face\nhf_logging.set_verbosity_error()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:37:34.667154Z","iopub.execute_input":"2025-08-24T00:37:34.667560Z","iopub.status.idle":"2025-08-24T00:40:30.043709Z","shell.execute_reply.started":"2025-08-24T00:37:34.667542Z","shell.execute_reply":"2025-08-24T00:40:30.042720Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¦ Instalando dependencias...\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m968.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m612.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.3.0 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.3.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2023.6.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **2. Preprocesamiento de textos**","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nAqui se define una funciÃ³n para limpiar y normalizar el texto, convirtiÃ©ndolo a minÃºsculas, eliminando stopwords y conservando solo aquellos signos de puntuaciÃ³n que son relevantes en el idioma espaÃ±ol (como exclamaciones e interrogaciones), permitiendo reducir el ruido en los datos y mejorar el rendimiento de los modelos posteriores.","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    \"\"\"Limpia el texto: minÃºsculas, conserva puntuaciÃ³n relevante, elimina stopwords.\"\"\"\n    text = text.lower()\n    # Conservar signos de exclamaciÃ³n e interrogaciÃ³n\n    text = ' '.join(word for word in text.split() if word not in stop_words)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.044940Z","iopub.execute_input":"2025-08-24T00:40:30.045284Z","iopub.status.idle":"2025-08-24T00:40:30.050223Z","shell.execute_reply.started":"2025-08-24T00:40:30.045245Z","shell.execute_reply":"2025-08-24T00:40:30.049500Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# **3. Cargar y explorar el Dataset**","metadata":{"id":"hTg_ajBlMqzH"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nEn esta parte se carga el dataset fake_news_corpus_spanish desde HuggingFace, se convierte en un DataFrame de Pandas y se aplican transformaciones iniciales como la conversiÃ³n de la variable CATEGORY a valores binarios y el preprocesamiento de los textos. AdemÃ¡s, se calculan estadÃ­sticas bÃ¡sicas de longitud de los textos y la distribuciÃ³n de las categorÃ­as, lo cual ayuda a detectar posibles desbalances en las clases.","metadata":{}},{"cell_type":"code","source":"def load_and_explore_dataset():\n    \"\"\"Carga el dataset y muestra estadÃ­sticas bÃ¡sicas.\"\"\"\n    print(\"ğŸ“¥ Cargando 'fake_news_corpus_spanish' dataset (split: test)...\")\n    try:\n        dataset = load_dataset(\"mariagrandury/fake_news_corpus_spanish\", split=\"test\")\n        df = dataset.to_pandas()\n    except Exception as e:\n        print(f\"Error al cargar el dataset: {e}\")\n        sys.exit(1)\n\n    # Convertir CATEGORY a binario y preprocesar textos\n    df['CATEGORY'] = df['CATEGORY'].astype(int)\n    df['TEXT'] = df['TEXT'].apply(preprocess_text)\n\n    # Mostrar informaciÃ³n del dataset\n    print(\"\\nColumnas del Dataset:\", df.columns.tolist())\n    print(\"\\nPrimeras Filas:\\n\", df.head())\n    print(\"\\nDistribuciÃ³n de CategorÃ­as:\\n\", df['CATEGORY'].value_counts())\n    print(f\"\\nTotal de Textos: {len(df)}\")\n\n    # EstadÃ­sticas de longitud de texto\n    text_lengths = [len(text) for text in df['TEXT']]\n    print(f\"Texto mÃ¡s corto: {min(text_lengths)}\")\n    print(f\"Texto mÃ¡s largo: {max(text_lengths)}\")\n    print(f\"Longitud promedio: {np.mean(text_lengths):.2f}\")\n\n    return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.052039Z","iopub.execute_input":"2025-08-24T00:40:30.052281Z","iopub.status.idle":"2025-08-24T00:40:30.072718Z","shell.execute_reply.started":"2025-08-24T00:40:30.052254Z","shell.execute_reply":"2025-08-24T00:40:30.072003Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# **4. Dividir el Dataset**","metadata":{"id":"ZyBrxkSlNSfT"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nUna vez cargado y explorado, el dataset se divide en subconjuntos para permitir un entrenamiento robusto y evaluaciones confiables. En esta secciÃ³n se define una funciÃ³n que separa el dataset en tres particiones las cuales son entrenamiento (80%), validaciÃ³n (10%) y prueba (10%).","metadata":{}},{"cell_type":"code","source":"def split_dataset(dataset):\n    \"\"\"Divide el dataset en conjuntos de entrenamiento, validaciÃ³n y prueba.\"\"\"\n    dataset_size = len(dataset)\n    train_size = int(0.8 * dataset_size)\n    val_size = int(0.1 * dataset_size)\n    test_size = dataset_size - train_size - val_size\n\n    train_subset, val_subset, test_subset = random_split(\n        dataset,\n        lengths=[train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n\n    print(f\"âœ… Train: {len(train_subset)}, Val: {len(val_subset)}, Test: {len(test_subset)}\")\n    return train_subset, val_subset, test_subset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.073389Z","iopub.execute_input":"2025-08-24T00:40:30.073558Z","iopub.status.idle":"2025-08-24T00:40:30.090718Z","shell.execute_reply.started":"2025-08-24T00:40:30.073544Z","shell.execute_reply":"2025-08-24T00:40:30.090014Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# **5. Clase de Dataset personalizado**","metadata":{"id":"sXoHrRHyNj1H"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nEn este punto se implementa una clase personalizada de Dataset para manejar el corpus de noticias falsas en espaÃ±ol. La clase se encarga de tokenizar los textos, aplicar truncamiento y padding hasta una longitud mÃ¡xima definida, y asociar cada texto con su etiqueta correspondiente.","metadata":{}},{"cell_type":"code","source":"class FakeNewsCorpusSpanishDataset(Dataset):\n    \"\"\"Dataset personalizado para clasificaciÃ³n de noticias falsas.\"\"\"\n    def __init__(self, tokenizer, dataset, seq_length=512):\n        self.tokenizer = tokenizer\n        self.dataset = dataset\n        self.seq_length = seq_length\n        self.id_2_class_map = {0: 'False', 1: 'True'}\n        self.class_2_id_map = {'False': 0, 'True': 1}\n        self.num_classes = len(self.id_2_class_map)\n\n    def __getitem__(self, index):\n        \"\"\"Tokeniza el texto preprocesado y retorna tensores.\"\"\"\n        text = preprocess_text(self.dataset[index]['TEXT'])\n        label = self.dataset[index]['CATEGORY']\n        label = self.class_2_id_map['True' if label == 1 else 'False']\n\n        tokenized = self.tokenizer(\n            text,\n            max_length=self.seq_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': tokenized['input_ids'].squeeze(0),\n            'attention_mask': tokenized['attention_mask'].squeeze(0),\n            'y': torch.tensor(label, dtype=torch.long)\n        }\n\n    def __len__(self):\n        return len(self.dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.091598Z","iopub.execute_input":"2025-08-24T00:40:30.092241Z","iopub.status.idle":"2025-08-24T00:40:30.105683Z","shell.execute_reply.started":"2025-08-24T00:40:30.092223Z","shell.execute_reply":"2025-08-24T00:40:30.104939Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **6. DefiniciÃ³n del modelo LSTM**","metadata":{"id":"XrDep4OXNsDL"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nAquÃ­ se define el modelo de clasificaciÃ³n basado en una red recurrente LSTM (Long Short-Term Memory). Este modelo utiliza embeddings para representar las palabras, capas LSTM bidireccionales para capturar dependencias en ambas direcciones del texto, normalizaciÃ³n por lotes para estabilizar el entrenamiento y capas totalmente conectadas con dropout para mejorar la generalizaciÃ³n.","metadata":{}},{"cell_type":"code","source":"class LSTMClassifier(nn.Module):\n    \"\"\"Clasificador basado en LSTM para detecciÃ³n de noticias falsas.\"\"\"\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n\n    def forward(self, input_ids):\n        embedded = self.embedding(input_ids)\n        lstm_out, _ = self.lstm(embedded)\n        pooled = lstm_out[:, -1, :]\n        pooled = self.batch_norm(pooled)\n        pooled = self.dropout(pooled)\n        logits = self.fc(pooled)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.106493Z","iopub.execute_input":"2025-08-24T00:40:30.106731Z","iopub.status.idle":"2025-08-24T00:40:30.123847Z","shell.execute_reply.started":"2025-08-24T00:40:30.106708Z","shell.execute_reply":"2025-08-24T00:40:30.123113Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# **7. MÃ³dulo de PyTorch Lightning**","metadata":{"id":"AElD5mgpN30y"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nPara simplificar el entrenamiento, validaciÃ³n y prueba del modelo, se crea un mÃ³dulo basado en PyTorch Lightning. Esta clase integra el modelo LSTM con la funciÃ³n de pÃ©rdida, optimizador, programador de tasa de aprendizaje y mÃ©tricas de evaluaciÃ³n. AdemÃ¡s, incluye mÃ©todos especÃ­ficos para cada fase del entrenamiento y predicciÃ³n, asegurando un flujo estandarizado y reproducible.","metadata":{}},{"cell_type":"code","source":"class SpanishNewsClassifierWithLSTM(pl.LightningModule):\n    \"\"\"MÃ³dulo de PyTorch Lightning para entrenar el clasificador LSTM.\"\"\"\n    def __init__(self, vocab_size, num_classes, embed_dim=256, hidden_dim=128, lr=3e-4):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes)\n        self.loss_fn = nn.CrossEntropyLoss()\n\n        # Inicializar pesos\n        for module in self.model.modules():\n            if isinstance(module, nn.LSTM):\n                nn.init.xavier_uniform_(module.weight_ih_l0)\n                nn.init.xavier_uniform_(module.weight_hh_l0)\n                nn.init.xavier_uniform_(module.weight_ih_l1)\n                nn.init.xavier_uniform_(module.weight_hh_l1)\n            elif isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n\n        # MÃ©tricas para clasificaciÃ³n binaria\n        self.train_acc = Accuracy(task='binary', num_classes=num_classes)\n        self.val_acc = Accuracy(task='binary', num_classes=num_classes)\n        self.test_acc = Accuracy(task='binary', num_classes=num_classes)\n        self.test_precision = Precision(task='binary', num_classes=num_classes)\n        self.test_recall = Recall(task='binary', num_classes=num_classes)\n        self.test_f1 = F1Score(task='binary', num_classes=num_classes)\n\n    def forward(self, input_ids):\n        return self.model(input_ids)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch['input_ids'], batch['y']\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        probs = logits.softmax(dim=-1)[:, 1]\n        self.train_acc(probs, y)\n        self.log('train_loss', loss, prog_bar=True)\n        self.log('train_acc', self.train_acc, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch['input_ids'], batch['y']\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        probs = logits.softmax(dim=-1)[:, 1]\n        self.val_acc(probs, y)\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', self.val_acc, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch['input_ids'], batch['y']\n        logits = self(x)\n        probs = logits.softmax(dim=-1)[:, 1]\n        self.test_acc(probs, y)\n        self.test_precision(probs, y)\n        self.test_recall(probs, y)\n        self.test_f1(probs, y)\n        self.log('test_acc', self.test_acc, prog_bar=True)\n        self.log('test_precision', self.test_precision, prog_bar=True)\n        self.log('test_recall', self.test_recall, prog_bar=True)\n        self.log('test_f1', self.test_f1, prog_bar=True)\n\n    def predict_step(self, batch, batch_idx):\n        x = batch['input_ids']\n        logits = self(x)\n        probs = logits.softmax(dim=-1)\n        return probs\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}\n        }\n\n    def on_train_batch_end(self, outputs, batch, batch_idx):\n        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.124642Z","iopub.execute_input":"2025-08-24T00:40:30.124882Z","iopub.status.idle":"2025-08-24T00:40:30.139361Z","shell.execute_reply.started":"2025-08-24T00:40:30.124858Z","shell.execute_reply":"2025-08-24T00:40:30.138563Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# **8. FunciÃ³n de PredicciÃ³n**","metadata":{"id":"xxa2Yt2LOCwv"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nCon el modelo ya entrenado, esta funciÃ³n permite evaluar textos nuevos para determinar si son verdaderos o falsos. Se aplica el mismo preprocesamiento y tokenizaciÃ³n que en el entrenamiento, y luego se obtiene la predicciÃ³n junto con la probabilidad asociada.","metadata":{}},{"cell_type":"code","source":"def predict_text(model, tokenizer, text, seq_length=512, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    \"\"\"Predice la clase de un texto nuevo.\"\"\"\n    model.eval()\n    model.to(device)\n    text = preprocess_text(text)\n    tokenized = tokenizer(\n        text,\n        max_length=seq_length,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n\n    input_ids = tokenized['input_ids'].to(device)\n    with torch.no_grad():\n        logits = model(input_ids)\n        probs = logits.softmax(dim=-1)\n        pred_class = torch.argmax(probs, dim=-1).item()\n\n    id_2_class = {0: 'False', 1: 'True'}\n    return id_2_class[pred_class], probs[0][pred_class].item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:40:30.140214Z","iopub.execute_input":"2025-08-24T00:40:30.140453Z","iopub.status.idle":"2025-08-24T00:40:30.180866Z","shell.execute_reply.started":"2025-08-24T00:40:30.140438Z","shell.execute_reply":"2025-08-24T00:40:30.180402Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# **9. EjecuciÃ³n Principal y Metricas de EvaluaciÃ³n del Modelo**","metadata":{"id":"Lh42TI7xOR7o"}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\nFinalmente, se organiza todo el flujo en una funciÃ³n principal. AquÃ­ se configuran los dispositivos de cÃ³mputo (CPU, GPU o TPU), se carga y divide el dataset, se inicializa el tokenizador y el modelo, y se definen los callbacks de entrenamiento como el EarlyStopping y la selecciÃ³n del mejor checkpoint. Tras el entrenamiento, el modelo se evalÃºa en el conjunto de prueba, se generan predicciones y se muestra un ejemplo de predicciÃ³n aplicada a un texto real.","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"FunciÃ³n principal para ejecutar el pipeline.\"\"\"\n    # Verificar disponibilidad de GPU/TPU\n    if IN_COLAB and torch.cuda.is_available():\n        device = torch.device('cuda')\n        print(\"CUDA Disponible: True\")\n        print(\"Nombre de GPU:\", torch.cuda.get_device_name(0))\n    elif IN_COLAB and 'XLA' in torch.__version__:\n        device = torch.device('xla')\n        print(\"TPU Disponible: True\")\n    else:\n        device = torch.device('cpu')\n        print(\"CUDA/TPU no disponible, usando CPU\")\n\n    # Cargar dataset\n    dataset = load_and_explore_dataset()\n\n    # Inicializar tokenizador\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n    except Exception as e:\n        print(f\"Error al cargar el tokenizador: {e}\")\n        sys.exit(1)\n\n    # Dividir dataset y almacenar Ã­ndices\n    train_subset, val_subset, test_subset = split_dataset(dataset)\n    test_indices = test_subset.indices\n\n    # Crear datasets personalizados\n    train_dataset = FakeNewsCorpusSpanishDataset(tokenizer, train_subset, seq_length=512)\n    val_dataset = FakeNewsCorpusSpanishDataset(tokenizer, val_subset, seq_length=512)\n    test_dataset = FakeNewsCorpusSpanishDataset(tokenizer, test_subset, seq_length=512)\n\n    # Crear dataloaders\n    batch_size = 16  # Reducir para gradientes mÃ¡s precisos\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=True)\n\n    # Inicializar modelo\n    model = SpanishNewsClassifierWithLSTM(\n        vocab_size=tokenizer.vocab_size,\n        num_classes=train_dataset.num_classes,\n        embed_dim=256,\n        hidden_dim=128,\n        lr=3e-4\n    )\n\n    # Configurar logger y callbacks\n    tb_logger = TensorBoardLogger('tb_logs', name='LSTMClassifier')\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=10, mode='min'),\n        ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, filename='best-checkpoint', dirpath='checkpoints')\n    ]\n\n    # Inicializar entrenador\n    trainer = pl.Trainer(\n        max_epochs=20,\n        accelerator=\"gpu\" if torch.cuda.is_available() else \"tpu\" if IN_COLAB and 'XLA' in torch.__version__ else \"cpu\",\n        devices=1,\n        logger=tb_logger,\n        callbacks=callbacks,\n        precision=\"16-mixed\" if torch.cuda.is_available() else \"16-true\" if IN_COLAB and 'XLA' in torch.__version__ else \"32-true\",\n        num_sanity_val_steps=0\n    )\n\n    # Entrenar modelo\n    print(\"ğŸš€ Iniciando entrenamiento...\")\n    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\n    # Evaluar en conjunto de prueba\n    print(\"ğŸ” Evaluando en conjunto de prueba...\")\n    trainer.test(model, dataloaders=test_loader)\n\n    # Generar DataFrame con predicciones\n    print(\"ğŸ“Š Generando DataFrame con predicciones...\")\n    model.eval()\n    predictions = trainer.predict(model, test_loader)\n    predictions = torch.cat(predictions, dim=0)\n    predictions = torch.argmax(predictions, dim=-1)\n    predictions = [train_dataset.id_2_class_map[pred.item()] for pred in predictions]\n\n    df = pd.DataFrame(data={\n        \"texto\": [dataset[i]['TEXT'] for i in test_indices],\n        \"categorÃ­a\": [train_dataset.id_2_class_map[dataset[i]['CATEGORY']] for i in test_indices],\n        \"predicciÃ³n\": predictions[:len(test_indices)]\n    }, index=test_indices)\n    print(df.head(15))\n\n    # Ejemplo de predicciÃ³n\n    sample_text = \"El presidente anunciÃ³ nuevas medidas econÃ³micas.\"\n    prediction, confidence = predict_text(model, tokenizer, sample_text)\n    print(f\"\\nğŸ“ PredicciÃ³n de Ejemplo: '{sample_text}' -> {prediction} (Confianza: {confidence:.4f})\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:46:02.036503Z","iopub.execute_input":"2025-08-24T00:46:02.036786Z","iopub.status.idle":"2025-08-24T00:46:30.220835Z","shell.execute_reply.started":"2025-08-24T00:46:02.036766Z","shell.execute_reply":"2025-08-24T00:46:30.220106Z"}},"outputs":[{"name":"stdout","text":"CUDA Disponible: True\nNombre de GPU: Tesla P100-PCIE-16GB\nğŸ“¥ Cargando 'fake_news_corpus_spanish' dataset (split: test)...\n\nColumnas del Dataset: ['ID', 'CATEGORY', 'TOPICS', 'SOURCE', 'HEADLINE', 'TEXT', 'LINK']\n\nPrimeras Filas:\n    ID  CATEGORY    TOPICS         SOURCE  \\\n0   1         1  Covid-19  El Economista   \n1   2         0  PolÃ­tica     El matinal   \n2   3         1  PolÃ­tica        El PaÃ­s   \n3   4         0  PolÃ­tica     AFPFactual   \n4   5         1  Sociedad   La Republica   \n\n                                            HEADLINE  \\\n0                       Covid-19: mentiras que matan   \n1  El Gobierno podrÃ¡ acceder a las IPs de los mÃ³v...   \n2  La comunidad musulmana catalana denuncia a Vox...   \n3                                               None   \n4  El censo poblacional 2018 tendrÃ¡ un costo de $...   \n\n                                                TEXT  \\\n0  control covid-19 sÃ³lo tema mÃ©dicos resto perso...   \n1  gobierno pedro sÃ¡nchez pablo iglesias encontra...   \n2  tres federaciones agrupan 90% mezquitas llevan...   \n3  dado conocer datos electorales preliminares pe...   \n4  primera fase censo virtual solo abril prÃ³ximo ...   \n\n                                                LINK  \n0  https://www.eleconomista.com.mx/opinion/Covid-...  \n1  https://www.elmatinal.com/espana-ultima-hora/e...  \n2  https://elpais.com/espana/elecciones-catalanas...  \n3                         https://perma.cc/GYE6-SPMB  \n4  https://www.larepublica.co/economia/el-censo-p...  \n\nDistribuciÃ³n de CategorÃ­as:\n CATEGORY\n1    286\n0    286\nName: count, dtype: int64\n\nTotal de Textos: 572\nTexto mÃ¡s corto: 191\nTexto mÃ¡s largo: 18973\nLongitud promedio: 2301.06\nâœ… Train: 457, Val: 57, Test: 58\nğŸš€ Iniciando entrenamiento...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0ab5c5aaa854b37987d3587d056f966"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"ğŸ” Evaluando en conjunto de prueba...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01d4466f76f744a09821914751d41461"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.7291666865348816    \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ”‚\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6285714507102966    \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ”‚\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.7333333492279053    \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ”‚\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    0.550000011920929    \u001b[0m\u001b[35m \u001b[0mâ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.7291666865348816     </span>â”‚\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6285714507102966     </span>â”‚\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.7333333492279053     </span>â”‚\nâ”‚<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     0.550000011920929     </span>â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"ğŸ“Š Generando DataFrame con predicciones...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bad2267d452440c1b0777f3cb36ba2c1"}},"metadata":{}},{"name":"stdout","text":"                                                 texto categorÃ­a predicciÃ³n\n296  Las vacunas contra la COVID-19 -13 de ellas en...      True      False\n336  Si eres de los que ama tomarse una copita de v...      True      False\n99   AsÃ­ lo revelÃ³ el estudio en el que participaro...      True       True\n516  Vacaciones de Semana Santa, buen tiempo, el ha...      True       True\n227  Varios vuelos con salida de Casablanca y desti...      True      False\n344  Se estÃ¡ haciendo viral tras recuperarse una gr...     False      False\n74   MASCARILLAS.... PREFERENTEMENTE PARA CARNAVAL ...     False      False\n207  El Laboratorio BiolÃ³gico Chino de Wuhan es en ...     False      False\n480  El Papa Francisco podrÃ­a estar enfermo de Coro...     False      False\n325  El expresidente del Gobierno Felipe GonzÃ¡lez h...     False       True\n174  Una veintena de jÃ³venes del entorno abertzale ...     False      False\n431  El embarazo es parte de nuestra sexualidad y v...      True       True\n198  FUE TAN FÃCIL CONVENCER A CASI UN MUNDO ENTERO...     False      False\n533  A unas horas que FÃ©lix Salgado Macedonio sea r...      True      False\n48   Desde el mes pasado, la regiÃ³n de La Laguna de...     False      False\n\nğŸ“ PredicciÃ³n de Ejemplo: 'El presidente anunciÃ³ nuevas medidas econÃ³micas.' -> False (Confianza: 0.6482)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# **10. Resultados**","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\n1. Los resultados obtenidos en la evaluaciÃ³n del modelo muestran un desempeÃ±o general aceptable, alcanzando una exactitud del 72.91%. Este valor indica que, en promedio, el modelo clasifica correctamente cerca de tres cuartas partes de los textos evaluados. La precisiÃ³n alcanzÃ³ un valor de 0.73, lo que evidencia que, cuando el sistema identifica una noticia como verdadera o falsa, en la mayorÃ­a de los casos acierta en dicha clasificaciÃ³n. \n\n<p style=\"font-size: 16px;\">\n2. Sin embargo, el recall obtenido fue de 0.55, lo que revela que el modelo no logra identificar de manera efectiva todas las noticias falsas presentes en el conjunto de prueba, dejando escapar una proporciÃ³n significativa de estas.\n\n<p style=\"font-size: 16px;\">\n3. El valor del F1-score, situado en 0.62, refleja un equilibrio moderado entre la precisiÃ³n y el recall, pero tambiÃ©n confirma la necesidad de fortalecer la sensibilidad del modelo para detectar noticias falsas sin sacrificar en exceso la precisiÃ³n alcanzada. Al observar los ejemplos en el DataFrame generado, se aprecia que, aunque en muchos casos el modelo logra predecir de forma correcta, existen situaciones en las que clasifica errÃ³neamente noticias con lenguaje ambiguo, sensacionalista o con contenido genÃ©rico, lo que impacta en el recall.Finalmente, al evaluar un texto de ejemplo â€”â€œEl presidente anunciÃ³ nuevas medidas econÃ³micasâ€â€”, el sistema lo clasificÃ³ como noticia falsa con una confianza de 64.82%. \n\n<p style=\"font-size: 16px;\">\n4. Este resultado ilustra tanto la capacidad del modelo para emitir predicciones sobre nuevos textos como la dificultad que enfrenta para interpretar contextos con baja carga informativa, donde aumenta el riesgo de errores de clasificaciÃ³n.\n","metadata":{}},{"cell_type":"markdown","source":"# **11. Conclusiones**","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 16px;\">\n1. Los resultados alcanzados muestran que el modelo constituye una base sÃ³lida para la detecciÃ³n automÃ¡tica de noticias falsas en espaÃ±ol, dado que logra un nivel de exactitud por encima del 70%, lo cual es competitivo para un prototipo inicial desarrollado con un corpus limitado.\n\n<p style=\"font-size: 16px;\">\n2. Se observa que el modelo prioriza la precisiÃ³n sobre el recall, lo que implica que es mÃ¡s confiable al momento de afirmar que una noticia es falsa, pero a costa de no detectar todas las que realmente lo son. Esto lo convierte en un clasificador conservador, Ãºtil en contextos donde los falsos positivos deben minimizarse.\n\n<p style=\"font-size: 16px;\">\n3. La brecha entre precisiÃ³n y recall evidencia la necesidad de mejorar la cobertura del sistema, particularmente para capturar con mayor eficacia noticias falsas que poseen estructuras lingÃ¼Ã­sticas mÃ¡s diversas o complejas. Esto sugiere que una mayor variedad en los datos de entrenamiento podrÃ­a beneficiar significativamente su rendimiento.\n\n<p style=\"font-size: 16px;\">\n4. El F1-score alcanzado indica que existe un balance moderado entre precisiÃ³n y recall, pero que aÃºn no es suficiente para un uso en escenarios crÃ­ticos. Se puede optar por la optimizaciÃ³n de hiperparÃ¡metros y la experimentaciÃ³n con arquitecturas mÃ¡s avanzadas, como transformers en espaÃ±ol.\n\n<p style=\"font-size: 16px;\">\n5. El anÃ¡lisis de ejemplos concretos en el DataFrame revela que el modelo tiende a cometer errores en textos cortos, ambiguos o con expresiones genÃ©ricas. Esto pone en evidencia la importancia de considerar estrategias de data augmentation, para ampliar la diversidad lingÃ¼Ã­stica en el entrenamiento.\n\n<p style=\"font-size: 16px;\">\n6. Si bien el prototipo actual presenta limitaciones, sus resultados confirman el potencial de aplicar tÃ©cnicas de aprendizaje profundo en la detecciÃ³n de noticias falsas. Con ajustes en los datos, mejoras en la arquitectura y un refinamiento del pipeline.","metadata":{}}]}